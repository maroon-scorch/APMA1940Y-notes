\documentclass{article}
\input{setup.tex}


\renewcommand*\contentsname{Table of Content}

\title{Wavelets and Applications}
\author{Mattie Ji}
\date{Spring 2023}
\setlength\parindent{0pt}

\begin{document}

\maketitle
These are lecture notes from \textbf{APMA 1940Y: Wavelets and Applications} with Professor Mark Ainsworth at Brown University for the Spring 2023 semester. The most up-to-date version of the notes are maintained under my GitHub \href{https://github.com/maroon-scorch}{repository}.\\

These notes are taken by Mattie Ji with gracious help and input from the instructor of this course. If you find any mistakes in these notes, please feel free to direct them via email to me or send a pull request on GitHub.\\

The notes are last updated \today.
\tableofcontents
\newpage

\section{Lecture 1 - 1/25/2023}
There's a limit of \textbf{18} people in this class, but there are only \textbf{14} seats...

\subsection{Syllabus}
\begin{itemize}
    \item Instructor: Mark Ainsworth
    \item Best way to reach the instructor is by email
    \item Lectures are MWF from 10:00 am to 10:50 am
    \item Office hours are by appointment
\end{itemize}

The existential question every math major faces:
\begin{itemize}
    \item Most of your math classes won't be useful in the real world - Professor Ainsworth used to study fluid mechanics in college, and it is VERY different from how they do it in industry... so why should you be studying mathematics?
    \item Well, mathematics studies things abstractly and allows us to solve problems faster. Even though engineers and physicists have a headstart, mathematicians can catch up really fast.
    \item But the interviewer won't stop you right there - they will ask you how you math degree helped you to accomplish this! 
    \item This is the point of this class supposedly
\end{itemize}

This class is a WRIT class! So what is the WRIT component for - in the class, you will identify some applications of wavelets that interest you, and you will work a project on it and give a presentation on the selected topic in class. Professor Ainsworth will cover the theory part, and you will do the application part...\\

Certain applications of wavelets:
\begin{itemize}
    \item The theory of Wavelets were all started in the 1990s.
    \item This is very important in the internet age! For instace, there's something called the \textbf{wavelet decomposition} of images that displays an image in finer and finer renditions. This is quite useful for data transmissions.
    \item You can use wavelets in seismic analysis
    \item A former student analyzed how to identify forgeries and watermarks in documents using wavelets
    \item Another student made a program that uses wavelets for music analysis
    \item Another student used wavelets for the stock market ... it didn't go too well
\end{itemize}

Wavelets are also really useful for theory!
\begin{itemize}
    \item In Fourier Analysis (we will talk about this quite a bit), there's a certain defect with the subject we will exhibit that wavelets do not have.
\end{itemize}

Problem sets are due $7$ days after the lecture it is assigned. You have to turn it before the START of the lecture, either in-person or electronically. You are supposed to work on your problem set yourself.\\

Lecture notes will not be handed out, unless you have a good reason to be absent. It's important to be able to take your own notes!

\subsection{Time-Frequency Analysis}

For the first 5 or 6 classes, we won't be knowing what a wavelet is! But we will try to build towards the subject. In order to do this, we will first show some other ways for signal analysis.\\

\begin{definition}
A signal is the set $\{f(t): t \in \Rbb\}$:
\begin{itemize}
    \item Typically we will assume $f$ is continuous, but they need not be.
    \item $f$ could be either real-valued or complex-valued.
\end{itemize}
There are a few data associated to a signal:
\begin{itemize}
    \item The \textbf{energy of signal} is $$||f|| \coloneqq (\int_{-\infty}^\infty |f(t)|^2 dt)^{1/2}$$ This is pronounced ``norm f". Note this varies linearly with $|f(t)|$
    \item The \textbf{center of signal} is $$t^* \coloneqq \frac{\int_{-\infty}^\infty t \cdot |f(t)|^2 dt}{\int_{\infty}^{\infty} |f(t)|^2 dt}$$ This roughly measures where $f$ is most of the time. The denominator makes sure that the center is invariant up to scaling. Note that a symmetric signal has the signal at the line of symmetry.
    \item The \textbf{radius of signal} is $$\Delta^2 \coloneqq \frac{\int_{-\infty}^\infty (t - t^*)^2 |f(t)|^2 dt}{\int_{\infty}^{\infty} |f(t)|^2 dt}$$  This is roughly a measure of how spread out a signal is, so we want the spread to be translation invariant! The denominator is again for scale invariance. The actual radius is $\Delta$, not $\Delta^2$.
\end{itemize}
\end{definition}

\begin{remark}
This has a lot of connections with probability and statistics! For example for any signal $f$, define
\[p(t) = \frac{|f(t)|^2}{||f||^2}\]
This is a probablity distribution as
\[\int_{-\infty}^\infty p(t) dt = 1\]
The mean is exactly the center
\[\mu = \int_{-\infty}^\infty t p(t) dt = t^*\]
The variance is exactly the radius
\[\sigma^2 = \int_{-\infty}^\infty (t - \mu)^2 p(t) dt = \Delta^2\]
\end{remark}

\newpage
\section{Lecture 2 - 1/27/2023}

\subsection{Time Frequency Analysis - Continued}

Let's test our measures on an example:

\begin{example}
    Define the signal $f(t)$ where
    \[f(t) = \begin{cases}
        \frac{1}{2\epsilon},\ -\epsilon \leq t \leq \epsilon\\
        0,\ \text{otherwise}
    \end{cases}\]
    where $\epsilon > 0$. We note that as $\epsilon \to 0$, $f$ is more localized. Hence,
    \[||f||^2 = \int_{-\infty}^{\infty} |f(t)|^2 dt = \int_{-\epsilon}^\epsilon (\frac{1}{2\epsilon})^2 dt = \frac{1}{2\epsilon} < \infty\]
    \[t^* = \int_{-\epsilon}^\epsilon t(\frac{1}{2\epsilon})^2 dt = 0\]
    \[\Delta^2 = \frac{1}{||f||^2} \int_{-\infty}^\infty t^2 |f(t)|^2 dt = \int_{-\epsilon}^\epsilon t^2 (\frac{1}{2\epsilon}) dt = \frac{\epsilon^2}{3}\]
    \[\Delta = \frac{\epsilon}{\sqrt{3}}\]
    We note that as $\epsilon \to 0$, $||f||^2 \to \infty$, $t^* = 0$, and $\Delta \to 0$ is more localized. As $\epsilon \to \infty$, $||f||^2 \to 0$, $t^* = 0$, and $\Delta \to \infty$ is less localized.
\end{example}

\begin{remark}
    The energy, center, radius, are really just integrals of $|f(t)|^2$ multiplied with $t^0$, $t^1$, and $t^2$ respectively (roughly). This is called the $0$-th, $1$-st, and $2$-nd moments of the function. Suppose we try to generalize this to higher moments for all $n \in \Nbb$, can we reconstruct the signal out of the moments?\\\\
    It turns out that the set of polynomial functions are dense in the continuous functions from $[a, b]$ to $\Rbb$ (Stone-Weierstrass Theorem), but you can't uniformly approximate, for example, $e^x: \Rbb \to \Rbb$ using polynomials.
\end{remark}

\subsection{Fourier Transform}

Given a signal $f$ with finite energy, what can we do with $f$ that tells us more information about it? Calculating all the moments is just unfeasible.\\

Let $\omega \in \Rbb$, consider the term
\[e^{-i\omega t} f(t)\]
This term extracts the frequencies of $f(t)$, consider the function in terms of $\omega$, $\hat{f(\omega)}$
\[\omega \mapsto \int_{-\infty}^\infty e^{-i\omega t} f(t) dt\]

\begin{definition}
    The function $\omega \mapsto \hat{f}(\omega)$ is the \textbf{Fourier Transform} of $f$. This is a continuous version of Fourier Series we have dealt with before (plugging in integers to this function extracts our Fourier Coefficients).
\end{definition}

\begin{remark}
    There might be some issue with the integral not existing. Since this is an Applied Math class, all integrals exist...
\end{remark}

% \begin{definition}
%     The Fourier coefficients are defined as
%     \[A_n = \int_0^{2\pi} cos(t) f(t) dt\]
% \end{definition}

Suppose we have what $\hat{f}(\omega)$ is for every $\omega$, can we recover what $f$ is?\\

Yes! This is the statement of the Fourier Inversion Theorem:

\begin{theorem}[Fourier Inversion Theorem]
    Let $\hat{f}(\omega) = \int_{-\infty}^\infty e^{-i\omega t} f(t) dt$, then
    \[f(t) = \frac{1}{2\pi} \int_{-\infty}^\infty e^{+i\omega t} \hat{f}(\omega) d\omega\]
    The transformation back is called the Fourier Inversion formula.
\end{theorem}

Heuristically, $\hat{f}(\omega)$ represents the frequencies of the signals, and $f(t)$ represents the time series of the signal.

\begin{proposition}[Parseval Identity]
Let $f(t), g(t): \Rbb \to \Cbb$, then
\[\int_{-\infty}^\infty f(t) \overline{g(t)}\ dt = \frac{1}{2\pi} \int_{-\infty}^\infty \hat{f}(\omega) \overline{\hat{g}(\omega)}\ d\omega\]
\end{proposition}

\begin{proposition}
    $\Hat{f'}(\omega) = i \omega \hat{f}(\omega)$ (For less confusion, this is Fourier transform of $f'$)
\end{proposition}

\begin{proof}
    Let $f(t)$, $t \in \Rbb$, consider $\hat{f'(t)}(\omega)$. Now recall that
        \[f(t) = \frac{1}{2\pi} \int_{-\infty}^\infty e^{+i\omega t} \hat{f}(\omega) d\omega\]
        Taking the derivative on both sides, we have that
        \begin{align*}
            f'(t) &= \frac{1}{2\pi}\int_{-\infty}^\infty \frac{d}{dt} e^{+i\omega t} \hat{f}(\omega) d\omega\\
            &= \frac{1}{2\pi}\int_{-\infty}^\infty i \omega e^{+i\omega t} \hat{f}(\omega) d\omega
        \end{align*}
        Applying the Fourier Inversion Formula to $\hat{f'}(\omega)$, we can also write
        \begin{align*}
            f'(t) &= \frac{1}{2\pi} \int_{-\infty}^\infty e^{i \omega t}  \hat{f'}(\omega) d\omega
        \end{align*}
        But by the Fourier Inversion Formula, we have that
        \[\frac{1}{2\pi} \int_{-\infty}^\infty e^{i \omega t} \underline{\hat{f'}(\omega)} d\omega = f'(t) = \frac{1}{2\pi}\int_{-\infty}^\infty e^{+i\omega t} \underline{i \omega \hat{f}(\omega)} d\omega\]
    Hence we have that
    \[\hat{f'}(\omega) = i \omega \hat{f}(\omega)\]
\end{proof}

\begin{example}
    Suppose $f'(t) = g(t)$ where $t \in \Rbb$ is in the time domain, hence we have that
    \[\frac{df}{dt} = g(t)\]
    What if we try to examine the frequency domain instead? Then consider taking the Fourier Transform on both sides:
    \[\hat{\frac{df}{dt}}(\omega) = \hat{g}(\omega)\]
    What is the Fourier Transform of a derivative? Well, we end up having that
    \[i \omega \hat{f} = \hat{g}\]
    Or equivalently we have
    \[\hat{f}(\omega) = \frac{1}{i\omega} \hat{g}(\omega)\]
    Computing the Fourier Inversion on both sides gives back our solution.
\end{example}

\begin{remark}
    Lars Hörmander wrote a $5$-volume book on PDES and dedicated Fourier Transformation to most of it.
\end{remark}

\newpage
\section{Lecture 3 - 1/30/2023}

Right now we have been going fairly slowly... Recall, so far, we have an underlying signal given either as
\begin{itemize}
    \item (Time Domain Representation) - $\{f(t)\ :\ t \in \Rbb\}$
    \item (Frequency Domain Representation) - $\{\hat{f}(\omega)\ :\ \omega \in \Rbb\}$
\end{itemize}
These are essentially two sides of the same ``coin" (the coin being the signal here).\\

In particular, we can convert time to frequency and backwards using the Fourier Transform:
\[\hat{f}(\omega) \coloneqq \int_{-\infty}^\infty e^{-iwt} f(t) dt\]
\[f(t) = \frac{1}{2\pi} \int_{-\infty}^\infty e^{+iwt} \hat{f}(\omega) d\omega\]

We also have this \textbf{Parseval Identity} that gives
\[\int_{-\infty}^\infty f(t) \overline{g(t)} dt = \frac{1}{2\pi} \int_{-\infty}^\infty \hat{f}(\omega) \overline{\hat{g}(\omega)} d\omega \]

\begin{corollary}
Choose $g = f$, then we have that
\[||f||^2 = \frac{1}{2\pi} ||\hat{f}||^2\]
This gives a relationship between the energy of the signal and the $L^2$ norm of the frequency domain.
\end{corollary}

\begin{example}[Frequency of the Step Function]
    Let $\epsilon > 0$ and define $f(t) = \begin{cases}
        \frac{1}{2\epsilon},\ t \in [-\epsilon, \epsilon]\\
        0,\ \text{otherwise}
    \end{cases}$. What is $\hat{f}(\omega)$ in this case?
    \begin{align*}
        \hat{f}(\omega) &= \int_{-\infty}^\infty f(t) e^{-i\omega t} dt\\
        &= \int_{-\epsilon}^\epsilon \frac{1}{2\epsilon} e^{-i\omega t} dt\\
        &= \frac{-1}{2i \omega \epsilon} [e^{i\omega \epsilon} - e^{-i\omega \epsilon}]\\
        &= \frac{1}{\omega \epsilon} \frac{e^{i\omega \epsilon} - e^{-i\omega \epsilon}}{2i}\\
        &= \frac{\sin(\omega \epsilon)}{\omega \epsilon},\ \omega \in \Rbb - \{0\}
    \end{align*}
    Note when $\omega = 0$, this approaches $1$. Hence we have
    \[\hat{f}(\omega) = \begin{cases}
        \frac{\sin(\omega \epsilon)}{\omega \epsilon},\ \omega \in \Rbb - \{0\}\\
        1,\ \omega = 0
    \end{cases}\]
    We note that while $f(t)$ is compactly supported in the time domain, its frequency IS NOT compactly supported in the frequency domain but it does decary as $\|\omega \epsilon\| \to \infty$.\\\\
    Having a frequency of $0$ means the signal's not changing at that $\omega$. In the language of Electrical Engineering, this means there's a DC component at these roots.\\\\
    $\hat{f}(\omega)$ is what's called a \textbf{sink function}! Someone wrote an entire book about how to use sink functions to approximate solutions to PDEs. The time function can be thought of as the linearly combination of all the frequencies (super-position).\\\\
    We also make the following notes:
    \begin{enumerate}
        \item As $\epsilon \to 0$, $f(t)$ becomes more localized. In other words, its spread $\Delta$ is getting smaller.
        \item As $\epsilon \to 0$, $\hat{f}(\omega)$ becomes less localized, its correspondent spread $\hat{\Delta}$ is getting larger.
        \item In other words, the more you know about the time localizations, the less you know about the frequency localization, and vice versa.
        \item As $\epsilon \to \infty$, $f(t)$ is less localized.
        \item As $\epsilon \to \infty$, $\hat{f}(\omega)$ decays faster and faster, and becomes more localizaed.
    \end{enumerate}
\end{example}

This observation is in fact true in general, which leads us to our next section:

\subsection{The Uncertainty Principle}

Let $\{f(t) : t \in \Rbb\}$ be real-valued and assume $|f(t)|$ and $|\hat{f}(\omega)|$ decay sufficiently fast that:
\begin{itemize}
    \item $||f|| < \infty$ (this implies $||\hat{f}||$ is finite)
    \item $t^*$ (center of time domain) and $\omega^*$ (center of frequency domain) are well-defined.
    \item $\Delta$ and $\hat{\Delta}$ are well-defined.
\end{itemize}

You are not going to win a Field's Medal for proving these, but they do provide connections between the topics:
\begin{proposition}
    If $f$ is real-valued, then $\omega^*$ is ALWAYS $0$.
\end{proposition}

\begin{proof}
        Indeed we note that
        \[\hat{f}(\omega) = \int_{-\infty}^\infty e^{-i\omega t} f(t) dt\]
        Taking the complex conjugate and since $f(t)$ is real valued
        \[\overline{\hat{f}(\omega)} = \int_{-\infty}^\infty e^{+\omega t} f(t) dt = \hat{f}(-\omega)\]
        Hence, we have that
        \[|\hat{f}(\omega)|^2 = \hat{f}(\omega) \overline{\hat{f}(\omega)} = \hat{f}(\omega) \hat{f}(-\omega)\]
        Hence the mapping $\omega \mapsto |\hat{f}(\omega)|^2$ is a even function. We know the average of an even function is just $0$, hence
        \[\omega^* \alpha \int_{-\infty}^\infty \omega |\hat{f}(\omega)|^2 d\omega = 0\]
\end{proof}

But this result tells us that knowing $\omega^* = 0$ gives us very little information about what $f(t)$ is.

\begin{question}
    What about $\hat{\Delta}$? Does this tell us anything?
\end{question}

Consider the following translation function of $f$
\[f_\alpha(t) \coloneqq f(t - \alpha), t \in \Rbb\]

\begin{proposition}
    Let $\hat{f}_\alpha(\omega)$ denote the Fourier Transform of $f_\alpha(t)$, then
    \[\hat{f}_\alpha(\omega) = e^{-i\omega \alpha} \hat{f}(\omega)\]
\end{proposition}

\begin{proof}
What is $\hat{f}_\alpha(\omega)$? Well
\begin{align*}
    \hat{f}_\alpha(\omega) &= \int_{-\infty}^\infty e^{-iwt} f(t - \alpha) dt\\
    &= \int_{-\infty}^\infty e^{-i\omega (s + \alpha)}f(s) ds \tag*{Let $s = t - \alpha$}\\
    &= e^{-i\omega \alpha} \int_{-\infty}^\infty e^{-i\omega s} f(s) ds\\
    &= e^{-i\omega \alpha} \hat{f}(\omega)
\end{align*}    
\end{proof}

Thus, translation in the time domain corresponds to a phase shift (ie. some rotation) in the frequency domain! 

\begin{proposition}
    Let $\hat{\Delta}^2_\alpha$ be the spread of $\hat{f}_\alpha$, then
    \[\hat{\Delta}^2_\alpha = \hat{\Delta}^2\]
    In other words, shfiting in the time domain has no effect on the spread of the frequency domain.
\end{proposition}

\begin{proof}
To compute $\hat{\Delta}^2_\alpha$, we have
\begin{align*}
    \hat{\Delta}^2_\alpha &= \int_{-\infty}^\infty (\omega - \omega^*)^2 |\hat{f}_\alpha(\omega)|^2 d\omega\\
    &= \int_{-\infty}^\infty (\omega)^2 |\hat{f}_\alpha(\omega)|^2 d\omega\\
    &= \int_{-\infty}^\infty (\omega)^2 |1| |\hat{f}(\omega)| d\omega\\ \tag*{$e^{iat}$ has complex norm of $1$}
    &= \int_{-\infty}^\infty (\omega)^2 |\hat{f}(\omega)| d\omega\\
    &= \hat{\Delta}^2
\end{align*}    
\end{proof}

$\hat{\Delta}_\alpha = \hat{\Delta}$ is true for all $\alpha$! So again, knowing $\hat{\Delta}$ tells us very little about the time domain. Knowing the mean and variance of the frequency domain tells us almost nothing about the time domain.

\begin{remark}
    It turns out that there's a \textbf{fundamental limit} on what we can know about the time domain representation and the frequency domain representation simultaneously.\\\\
    There is in fact a quantative statement about this, let $f(t)$ be a function, then
    \[\Delta_f \hat{\Delta}_f \geq \frac{1}{2}\]
    This is known as the \textbf{Time-Frequency Uncertainty Principle}.
\end{remark}

\newpage
\section{Lecture 4 - 2/1/2023}

\subsection{Cauchy-Schwarz For Integrals}

\begin{theorem}[Cauchy-Schwarz for Integrals]
Let $f, g$ be real-valued functions then
\[(f, g) \leq ||f|| ||g||\]
where $(f, g) \coloneqq \int f g$.
\end{theorem}

\begin{proof}
Let $f, g$ be real-valued and let $\lambda \in \Rbb$, then
\[0 \leq \int (f - \lambda g)^2\]
Or equivalently we have that
\[0 \leq ||f||^2 - 2 \lambda (f, g) + \lambda^2 ||g||^2\]
This is really a quadratic equation in variable $\lambda$. Now hoose $\lambda$ to be the value that minimizes the expression $||f||^2 - 2 \lambda (f, g) + \lambda^2 ||g||^2$. In deed, this is achieved when
    \[\lambda = \frac{(f, g)}{||g||^2}\]
    So we have that
    \[0 \leq ||f||^2 - \frac{2 (f, g)^2}{||g||^2} + \frac{(f, g)^2}{||g||^2}\]
    Equivalently we have that
    \[(f, g)^2 \leq ||f||^2 \cdot ||g||^2\]
\end{proof}

\subsection{Deriving the Time Frequency Uncertainty Principle}

\begin{theorem}
Let $f$ be real-valued signal (note the result still holds for complex valued signal), then
\[\Delta_f \widehat{\Delta}_f \geq 1/2\]    
\end{theorem}

\begin{proof}
    If $f$ doesn't decay fast enough, the inequality on the left side would be infinite. Otherwise, suppose $f$ does decay fast enough, consider $||f||^2 = \int_{-\infty}^\infty |f(t)|^2 dt$. Let $u = |f(t)|^2$ and $dv = 1$, then $v = (t - t^*)$, and integration by parts gives
   \begin{align*}
       ||f||^2 &= \int_{-\infty}^\infty |f(t)|^2 dt\\
       &= [(t - t^*) |f(t)|^2]_{-\infty}^\infty - 2\int_{-\infty}^\infty (t - t^*) f(t) f'(t) dt
   \end{align*}
   Since $t^*$ s finite, then $(t - t^*) |f(t)|^2$ is integrable, so we have that the first term $[(t - t^*) |f(t)|^2]_{-\infty}^\infty = 0$. So we have that
   \begin{align*}
       ||f||^2 &= |-2 \int_{-\infty}^\infty (t - t^*) f(t) f'(t) dt|\\
       &\leq 2\sqrt{\int_{-\infty}^\infty (t - t^*)^2 |f(t)|^2 dt} \sqrt{\int_{-\infty}^\infty |f'(t)|^2 dt} \tag*{Cauchy Schwartz Inequality}\\
       &= 2(\Delta_f ||f||) \sqrt{\int_{-\infty}^\infty |f'(t)|^2 dt}
   \end{align*}
   What is $\int_{-\infty}^\infty |f'(t)|^2 dt$? Well, recall we have previously proven that $\widehat{f'(\omega)} = i \omega \widehat{f}(\omega)$, then Parserval's Identity tells us that
   \begin{align*}
       \int_{-\infty}^\infty |f'(t)|^2 dt &=  \frac{1}{2\pi} ||i \omega \widehat{f}(\omega)||^2\\
       &= \frac{1}{2\pi} \int_{-\infty}^\infty (\omega)^2 |\widehat{f}(\omega)|^2 d\omega\\
       &= \frac{1}{2\pi} \int_{-\infty}^\infty (\omega - 0)^2 |\widehat{f}(\omega)|^2 d\omega\\
       &= \frac{1}{2\pi} \widehat{\Delta}_f^2 ||\widehat{f}||^2 \tag*{Since $\omega^* = 0$ as $f$ is real valued}\\
       &= \widehat{\Delta}_f^2 ||f||^2
   \end{align*}
   Thus, we have that
   \[||f||^2 \leq 2 \Delta_f ||f|| \widehat{\Delta}_f ||f||\]
   Hence we have that
   \[\Delta_f \widehat{\Delta}_f \geq 1/2\]
\end{proof}

\begin{proposition}[Principle of Maximum Laziness]
    Mark Ainsworth's favorite principle - avoid all work at as much as possible.
\end{proposition}

\begin{remark}
    Note that the inequality is a very tight inequality! Since the only part where it becomes an inequality is when we applied to Cauchy-Schwarz Inequality. Hence
    \[1/2 = \Delta_f \widehat{\Delta}_f\]
    if and only if the condition for Cauchy-Schwarz Inequality to become an equality is true. THis happens when $(t - t^*) f(t)$ and $f'(t)$ differs by a constant! This probably gives you an idea of what function you can use to make this an equality.
\end{remark}

The mathematical proof of the Uncertainity Principle isn't too bad, what's tricky is how we would interpret this in the context of applications.\\

In the context of Heisenberg's Uncertainty Principle, we have that
\[\Delta_f \sim \text{position of particle}\]
\[\widehat{\Delta}_f \sim \text{momentum of particle}\]
Up to some appropriate scaling, we have that the constant that bounds below the product is the Planck's constant. This really has nothing to do with physics but rather a consequence of the Cauchy-Schwarz Inequality.\\

In the context of time-frequency analysis, this means that the more narrow you examine in the time domain, the less you know about how the frequency at the place works, and vice versa. This is a HUGE problem in signal processing. You know everything with Fourier Analysis if you have what $f(t)$ or $\widehat{f}(\omega)$ are, but in the real world, you don't know what $f(t)$ or $\widehat{f}(\omega)$ are, you only have localized data about the waves.

\subsection{Bandwidths and Filtering}

Suppose we have a signal with time domain representation $\{f(t): t \in \Rbb\}$ with $f$ being real-valued.\\

Therefore, the frequency representation
\[\widehat{f}(\omega) = \int_{-\infty}^\infty e^{-i\omega t} f(t) dt\]
Ideally a frequency representation would capture all the frequencies. Unfortunately, due to limitations in the real world, we can't do that, our machines just can't measure low and high frequencies up to some number, so we are dealing with the following type of signals:

\begin{definition}
    A signal $f(t)$ is \textbf{band limited} if 
    \[\widehat{f}(\omega) = 0,\ \forall \omega \text{ such that } |\omega| \geq \alpha > 0\]
    The smallest of such $\alpha$ is the \textbf{bandwidth} of the signal.
\end{definition}

\begin{example}
    Given $\alpha > 0$, we set $$\widehat{f}_\alpha(\omega) \coloneqq \begin{cases} \widehat{f}(\omega),\ |\omega| \leq \alpha\\ 0,\ |\omega| > \alpha \end{cases}$$, then $\widehat{f}_\alpha(\omega)$ is band limited with bandwidth $\alpha$.
\end{example}

\begin{question}
    What can we infer about $f(t)$ given $\widehat{f}(\omega)$?
\end{question}

\newpage
\section{Lecture 5 - 2/3/2023}

\subsection{Bandwidths and Filtering}

Given a signal $\{f(t)\ :\ t \in \Rbb\}$ or $\widehat{f}(\omega)\ :\ \omega \in \Rbb$, we recall that we said the signal is \textbf{band-limited} with \textbf{bandwidth $\alpha$} if
\[\widehat{f}(\omega) = 0,\ \forall |\omega| > \alpha\]
where $\alpha$ is the smallest such value.\\\\

Given a frequency $\widehat{f}(\omega)$ and $\alpha > 0$, we can construct the \textbf{cut-off frequencies} at $\alpha$ as:
\[\widehat{f}_\alpha(\omega) = \begin{cases}
\widehat{f}(\omega),\ |\omega| < \alpha\\
0,\ |\omega| > \alpha
\end{cases}\]

\begin{question}
    What happens in time domain? What can we infer about $f(t)$ based on $\widehat{f}_\alpha(\omega)$.
\end{question}

We observe that $\widehat{f}_\alpha(\omega)$ decreases the radius of the frequency space and localizes this, by the Uncertainty Principle, this should intuitive make the time domain less localized.

\begin{remark}
    A negative frequency really just represents a wave in another direction. In the interpretation in its Fourier transform, a negative and positive frequency are respectively $e^{-it}$ and $e^{it}$, their sum is exactly $\cos(t)$, which is what we interpret to be as the standing wave.
\end{remark}

Assuming all signals decay fast enough, we observe that
\begin{align*}
    f_a(t) &= \frac{1}{2\pi} \int_{-\infty}^\infty \widehat{f}_a(\omega) e^{i\omega t} d\omega \\
    &= \frac{1}{2\pi} \int_{-a}^a \widehat{f}(\omega) e^{i\omega t} d\omega\\
    &= \frac{1}{2\pi} \int_{-a}^a e^{i\omega t} d\omega \int_{-\infty}^\infty e^{-i\omega s} f(s) ds\\
    &= \frac{1}{2\pi} \int_{-\infty}^\infty f(s) ds \int_{-a}^a e^{i\omega (t-s)} dt \tag*{Fubini's Theorem}
\end{align*}
We observes that
\[\frac{1}{2\pi} \int_{-a}^a e^{i\omega (t-s)} dt = \frac{1}{2\pi} [\frac{e^{i\omega(t-s)}}{i(t-s)}]_{-a}^a = \frac{1}{\pi(t-s)} \frac{e^{ia(t-s)} - e^{-a(t-s)}}{2i} = \ell_a(t - s)\]
We define
\[\ell_a(t) = \frac{\sin a t}{\pi t}\]
as the \textbf{Shannon Sampling Function}.

Thus, we have that
\[f_a(t) &= \int_{-\infty}^\infty f(s) \ell_a(t - s) ds = (f*\ell_a)(t)\]
This is indeed the convolution of $f(t)$ and $\ell_a(t)$.
\begin{itemize}
    \item Every value of $f(s)$ affects $f_a(t)$, apart from when $\sin \alpha t = 0$ (which is a discrete set)
    \item $f_a(t)$ is most affected by $f(t)$
\end{itemize}

\begin{question}
    What happens if we vary the value of $\alpha$? 
\end{question}

For $\alpha$ getting larger, we should expect that $\widehat{f}_a(\omega)$ gets closer and closer to $\widehat{f}(\omega)$, so $f_a(t)$ should be closer to $f(t)$. In fact, $\ell_a(t)$ becomes more and more like the Dirac Delta function as $a \to \infty$.\\

For $\alpha$ getting smaller, we should expect $f_a(t)$ getting less and less localized.\\

We also have a corresponding formulation of filtering by considering the time signal $f(t)$ and
\begin{definition}
We have a corresponding formulation of filtering - $\widehat{f} \to \widehat{f}_\alpha(\omega)$, where we can define
\[\widehat{f}_\alpha(\omega) \coloneqq \widehat(\omega) H(\omega)\]
where $H(\omega)$ is the indicator function of $[-\alpha, \alpha]$. $H$ is called the \textbf{frequency representation of the filter}.
\end{definition}

In the Frequency Domain, we have that
\[\widehat{f} \mapsto \widehat{f}_a = \widehat{f} H\]
In the Time Domain, we have that
\[f \mapsto f_a = f*h\]
where we note that $H = \widehat{h}$!

\begin{remark}
    So who is Claude Shannon? His contribution to signal processing was figuring out - given a bandlimited signal, how many points do you need to sample, so that others can reconstruct the signal based on the points. This is sometimes called the Shannon Sampling Theorem, and we will discuss more about this later.
\end{remark}

\subsection{Filter Design}

Recall we have $\widehat{f} \to \widehat{f}_a(\omega) = H(\omega) \widehat{f}(\omega) \sim \widehat{f}(\omega)$.\\

How should we choose an appropriate $H(\omega)$? Well we ideally want
\begin{itemize}
    \item We know that $\widehat{f}(0) = \int_{-\infty}^\in f(t) dt$ (average value of $f$), hence we want $f_a$ and $f$ to have the same average, hence
    $$\widehat{f}_a(0) = H(0) \widehat{f}(0)$$
    This is true if and only if $H(0) = 1$.
    \item We want to perserve moments as before, so we want to perserve higher order moments, for example $\int_{-\infty}^\infty t f(t) dt$.\\
    We observe that
    \[\widehat{f}(\omega) = \int_{-\infty}^\infty e^{i\omega t} f(t) dt\]
    Thus we have that
    \[\frac{d\widehat{f}}{d\omega}(\omega) =\int_{\infty}^\infty -it e^{i\omega t} t f(t) dt\]
    Hence we have that
    \[\widehat{f}'(0) = \int_{-\infty}^\infty t f(t) dt\]
    So we really want to perserve $\widehat{f}_a'(0) = \widehat{f}'(0)$,  which we have
    \[\widehat{f}_a'(0) = H(0) \widehat{f}'(0) + H'(0) \widehat{f}(0)\]
    The equality we want is perserved if and only if $H(0) = 1$ and $H'(0) = 0$.
    \item By Engineering's Induction, (you can also generalize this rigorously), we have that $H$ perserves all momemnts up to order $m$ if and only if
    \[H(0) = 1,\ H'(0) = H''(0) = ... = H^{(m)}(0) = 0\]
    This type of filter is called a \textbf{BUTTERWORTH FILTER} of order $m$.
\end{itemize}

\newpage
\section{Lecture 6 - 2/6/2023}

\subsection{Gábor Transform}

Recall in last lecture, we have previously demonstrated the following equivalence:\\

Given a function $f$ in the time domain, where $h$ is a real-valued filter with the assumption:
\begin{itemize}
    \item $h$ is real-valued
    \item $t^* = 0$
    \item $\Delta_h < \infty$
\end{itemize}
Then what get's filtered in the time domain is
\[g = h * f\]

In the frequency world, we achieve the same with $\widehat{g} = H \widehat{f}$, where $H = \widehat{h}$ (Note our convolution becomes multiplication).\\

We also have the uncertainty principle, which tells us that there's a limit on the localization we can get simultaneously in time and frequency. What does this mean for filtering?\\

We have
\[g(\alpha) = \int_{-\infty}^\infty f(t) h(\alpha - t) dt,\ \alpha \in \Rbb\]
where $h(\alpha - t)$ representations the localization around $\alpha$.\\

We know the Fourier Transform of $h(\alpha - t)$ is just 
\begin{align*}
    \int_{-\infty}^\infty e^{-i\omega t} h(\alpha - t) dt &= \int_{-\infty}^\infty e^{-\omega (w - s)} h(s) ds \tag*{Let $s = \alpha - t$}\\
    &= e^{-i\omega \alpha} \overline{\int_{-\infty}^\infty e^{-i\omega s} h(s) ds}\\
    &= e^{-i\omega \alpha} \overline{H(\omega)}
\end{align*}

Parserval's Identity tells us that
\[\int_{-\infty}^\infty f(t) h(\alpha - t) dt = g(\alpha) = \frac{1}{2\pi} \int_{-\infty}^\infty \widehat{f}(\omega) e^{i\omega \alpha} H(\omega) d\omega\]
In the time domain, we have the localization of $f(t)$ around $t = \alpha$ over a radius $\Delta_h$ (the role of $h(\alpha - t)$ is the sampler).\\

In the frequency domain, this is the sampling of $\widehat{f}$ over a radius $\widehat{\Delta}_h$ centered around $\omega = 0$.\\

What happens in the time-frequency domain:
\[\includegraphics[width=0.5\textwidth]{Figures/lec6-1.png}\]
This is called the Time-Frequency Window on $(\alpha - \Delta_h, \alpha + \Delta_h) \times (-\widehat{\Delta}_h, \widehat{\Delta}_h)$. In other words, filtering gives information about $f(t)$ and $\widehat{f}(\omega)$ over a time-frequency window.\\

We note that the area of this window is bounded by
\[(2 \Delta_h)(2 \widehat{\Delta}_h) \geq 2\]

We have two significant issues:
\begin{itemize}
    \item The area of the time-frequency window is limited
    \item We have flexibility to shift the time-window by varying $\alpha$, but there's no flexibility to vary the frequency as it is always centered on $\omega = 0$.
\end{itemize}

\begin{question}
    Can we try to limit the area of the time-frequency window?
\end{question}

\begin{example}
    Here are some possible attempts:
    \begin{enumerate}
        \item Try $h(t) = \begin{cases} 
        \frac{1}{2\epsilon},\ |t| \leq \epsilon\\
        0,\ \text{otherwise}
        \end{cases}$, then $\widehat{h}(\omega) \sim \frac{\sin(\omega t)}{\omega t}$. Then we have that
        \begin{align*}
            \widehat{\Delta}_h &= \lim_{R \to \infty} \int_{-\infty}^\infty |\omega| |\widehat{f}(\omega)|^2 d\omega\\
            &= \infty
        \end{align*}
        So the radius is not good.
        \item Let's try the Gaussian filter $h(t) = e^{-t^2/2}$, then $\Delta_h \cdot \widehat{\Delta}_h = 1/2$. This suggests that a Gaussian filter should be optimal, but this function is globally supported. We do have rapid decay, and
        \[h(\alpha - t) = e^{-(t-\alpha)^2/2} \approx 0 \text{ when $|t - \alpha|$ is ``large"}\]
        In practice, we would just approximate using a Gaussian filter.
    \end{enumerate}
\end{example}

\begin{question}
    Can we try to move the time-frequency window vertically so that it can be centered on a given frequency $\xi$?
\end{question}

\begin{proof}[Answer]
    Yes! This is solved by what's known as the Gabor transform. Gabor actually won a Nobel Prize for discovering holograms. He was also a member of Fellows of the Royal Society.
\end{proof}

Now, let's consider
\[g(t) = (h * f)(t) \text{ or } \widehat{g}(\omega) = H(\omega) \widehat{f}(\omega)\]
given information around $\omega = 0$.

\begin{idea}
Let's try to shift the spectrum of $f$ before filtering!
\end{idea}

Recall we have that
\[f(s) =\frac{1}{2\pi} \int_{-\infty}^\infty e^{i\omega s} \widehat{f}(\omega) d\omega\]
This is a super-position of frequencies. Hence
\begin{align*}
    e^{-i\xi s} f(s) &= \frac{1}{2\pi} \int_{-\infty}^\infty e^{i(\omega - \xi)s} \widehat{f}(\omega) d\omega\\
    &= \frac{1}{2\pi} \int_{-\infty}^\infty e^{i\omega s} \widehat{f}(\omega + \xi) d\omega
\end{align*}
This shows that the spectrum of $f$ is shifted by an amount $\xi$ (sine $\widehat{f}(\omega) \to \widehat{f}(\omega + \xi)$).\\

\begin{definition}
Applying the filter to the resulting function gives the \textbf{Gabor transform}:
\[G_h(\alpha, \xi) \coloneqq (h* e^{-i\xi s}f(s))(\alpha) = \int_{-\infty}^\infty e^{-i\xi s} f(s) h(\alpha - s) ds\]
\end{definition}

In the time-frequency window, we now have
\[\includegraphics[width=0.5\textwidth]{Figures/lec6-2.png}\]

This is also a special case of what's known as a ``short time Fourier transform".

\newpage
\section{Lecture 7 - 2/8/2023}

Recall last lecture - given a special filter $\{h(t)\ :\ t \in \Rbb\}$, then we define the Gabor transform as
\[(G_h f)(\alpha, \xi) = \int_{-\infty}^\infty ds f(s) e^{-i\xi s} h(\alpha - s)\]
The idea is that - by varying $\alpha$ and $\xi$, we can shift the window horizontally and vertically across the time-frequency domain, around spatial location $\alpha$ and frequency $\xi$.

\begin{proposition}
    The Gabor transform gives a time-frequency window
    \[(\alpha - \Delta_h, \alpha + \Delta_h) \times (\xi - \widehat{\Delta}_h, \xi + \widehat{\Delta}_h)\]
    ie. window has area
    \[2 \Delta_h 2 \widehat{\Delta}_h \geq \frac{2 \cdot 2}{2} = 2\]
\end{proposition}

\begin{proof}
    We note that $h(\alpha - s)$ is centered on $\alpha$ with spread being $\Delta_h$ and $e^{-i\xi s}$ has no contribution to the time domain, thus clearly the temporal window is given by $(\alpha - \Delta_h, \alpha + \Delta_h)$.\\\\
    By Parserval's Identity, we note that
    \[(G_h f)(\alpha, \xi) = \frac{1}{2\pi} \int_{-\infty}^\infty d\omega \widehat{f}(\omega) \overline{?}\]
    where the term under the complex conjugation is exactly the Fourier transform of $e^{+i \omega s} h(\alpha - s)$, which is
    \begin{align*}
        \int_{-\infty}^\infty e^{-i\omega s} e^{i \xi s} h(\alpha - s) ds &= \int_{-\infty}^\infty e^{-i(\omefa - \xi)s} h(\alpha - s) ds\\
        &= \int_{-\infty}^\infty e^{-i(\omega - \xi)\alpha} e^{i(\omega - \xi)t} h(t) dt \tag*{Let $t = \alpha - s$}\\
        &= e^{-i(\omega - \xi)\alpha} int_{-\infty}^\infty e^{-i(\xi - \omega)t} h(t) dt\\
        &= e^{-i(\omega - \xi)\alpha} \widehat{h}(\xi - \omega)
    \end{align*}
    Thus, we have that
        \[(G_h f)(\alpha, \xi) = \frac{1}{2\pi} \int_{-\infty}^\infty d\omega \widehat{f}(\omega) e^{+i(\omega - \xi)\alpha} \overline{\widehat{h}(\xi - \omega)}\]
        Hence, we have centering on $\omega = \xi$ on the frequency domain and width of window $\widehat{\Delta}_h$.
\end{proof}

\begin{example}
    Consider a signal $\{f(t) = 1\ :\ t\in \Rbb\}$ and $h(t) = e^{-t^2/2}$, then
    \begin{align*}
        (G_h f)(\alpha, \xi) &= \int_{-\infty}^\infty dt f(t) e^{-i\xi t} e^{-(t-\alpha)^2/2}\\
        &= e^{-i\xi \alpha} \int_{-\infty}^\infty e^{-(t-\alpha)^2/2 - i \xi t}\\
        &\sim e^{-i\xi \alpha} e^{-\xi^2/2}
    \end{align*}
    In particular, we note that $|G_h f| = e^{-\xi^2/2}$ with the graph:
    \[\includegraphics[width=0.5\textwidth]{Figures/lec7-1.png}\]
    Just analyzing the graph, we see that $|G_h f|$ is not varying in intensity in time, so there's no spatial localization at all. $|G_h f|$ is largest when $\xi = 0$, so the largest frequency content of the signal is around where $\xi = 0$, hence the dominant frequency is $\xi = 0$.
\end{example}

\begin{question}
    How to compute/approximate $G_h f$ when $f$ is not so simple?
\end{question}

We always make $h(t) = e^{-t^2/2}$ as our filter, to compute a numerical approximation, here are a few issues:
\begin{enumerate}
    \item $f(t)$ may be given only partially, e.g. only at certain points or on a limited range. Suppose we have $\{f(t): -w < t < w\}$ for some $w$. So the first approximation we need is
    \[(G_h f)(\alpha, \xi) \approx \int_{-w}^w dt f(t) e^{-i\omega t} e^{-(t- \alpha)^2/2}\]
    This is a good approximation if $f(t)$ decays rapidly outside. We also expect this to be a good approximation even if $f$ does not decay, provided that $\alpha$ is located ``well-inside" $(-w, w)$.
    \item We still can't compute analytically. Let's try to use the Trapezoidal Rule with nodes indexed by $-N, -N+1,..., 0, ..., N$ (choose $N$ large enough), then
    \[\approx \frac{W}{N} \sum_{m = -N}^N '' e^{-i \xi t_m} e^{-(t_m - \alpha)^2/2} f(t_m)\]
    where $t_m = m W/N$ for $m = -N, ..., N$ (the $\sum''$ means the first and the last term are halved).
    \item This could be very expensive if $N >> 1$ and/or $W >> 1$, we note that
    \[e^{-t^2/2} < 10^{-6} \text{ when } |t| > 5.6\]
    Therefore we are only going to sum over $N$ such that $|t_m - \alpha| < 6$, ie. only when
    \[-6N/W < m - \frac{N\alpha}{W} < 6N/W\]
    Hence combining our approximations gives
    \[(G_h f)(\alpha, \xi) \approx \frac{W}{N} \sum_{m = Lo}^{m = Hi}  e^{-i \xi t_m} e^{-(t_m - \alpha)^2/2} f(t_m)\]
    where
    \[Lo \coloneqq \max(-N, \lfloor frac{N}{W}(\alpha - 6) \rfloor)\]
    \[Hi = \min(N, \lceil N/W(\alpha + 6) \rceil)\]
    \[t_m = mW/N\]
    % \mattie{Fix this later }
\end{enumerate}

\newpage
\section{Lecture 8 - 2/10/2023}

\subsection{Gabor Transform (Continued)}
We recall that the Gabor transform was defined
\[(G_h f)(\alpha, \xi) = \int_{-\infty}^\infty f(t) e^{-i\xi t} e^{-(t- \alpha)^2/2} dt\]

The Gabor Transform enable us to do time-frequency analysis. In particular we can plot the following \textbf{spectrogram} and center on some window $(\alpha, \xi)$ with width $2\Delta_h$ and height $2 \widehat{\Delta}_h$:
\[\includegraphics[width=0.5\textwidth]{Figures/lec6-2}\]
But since we have this fundamental restriction on the area of the window,
$(G_h f)(\alpha, \xi) \sim $ average of $f$ and $\hat{f}$ over the window.\\\\

\begin{example}
   We can't concentrate this around points because of the fundamental limit on the area of the window, hence the spectrogram has some imperfections going on:
       \[\includegraphics[width=0.3\textwidth]{Figures/lec8-1}\]
   We can compare this with a music sheet. In particular, a music sheet is really an example of a spectrogram and is in fact a perfect spectrogram, as you know exactly what frequencies should occur at each time.
\end{example}

Unfortunately, Gabor transform is limited by the time-frequency uncertainty principle, but we can change the aspect ratio of the time-frequency window to focus on localization in either time or frequency.\\

To decrease $\Delta_h$, we want to decay our $e^{-t^2/2}$ faster, to do this we can consider $e^{-\epsilon t^2/2}$. If we choose $\epsilon > 1$, we tighten the time window. If $\epsilon < 1$, we tighten the frequency window.\\

A discontinuity at time $t$ corresponds to the existence of all frequencies around time $t$.

\begin{claim}
Varying $\epsilon$ is more useful than varying $\xi$ and/or $\alpha$.
\end{claim}

\begin{question}
    Why don't we try to sacrifice a parameter?
\end{question}

\subsection{Varying $t-\omega$ Aspect Ratio Transform}

Let $\{h(t): t \in \Rbb\}$ be a given filter and define a new transform
\[W_h(\alpha, a) = \int_{-\infty}^\infty f(t) h(\frac{\alpha - t}{a}) dt\]
By varging 'a', we can change the aspect ratio of the window (we are not normalizing yet)
\[\Delta_h^2 \sim \int_{-\infty}^\infty (t - t^*) |h(t)|^2 dt\ \ (a = 1)\]
\[\Delta_{h, a}^2 \sim \int_{-\infty}^\infty (t - t^*)^2 |h(t/a)|^2 dt\]

Without loss of generality, let's say $t^* = 0$, then
\begin{align*}
    \Delta_{h, a}^2 &\sim \int_{-\infty}^\infty t^2 |h(t/a)|^2 dt\\
    &=a^3 \int_{-\infty}^\infty (t/a)^2 |h(t/a)|^2 d(t/a)\\
    &= a^3 \int_{-\infty}^\infty s^2 |h(s)|^2 ds
\end{align*}
Recall we also have to normalize this, and
\[||h(t/a)||^2 = a \int_{-\infty}^\infty |h(t/a)|^2 \frac{dt}{a} = a ||h||^2\]
Hence, we have that
\[\Delta_{h, a}^2 \sim a^2 \Delta_h^2\]

Let's try to rescale $h$ so that $h(\frac{\bullet}{a})$ is independent of $|a|$, ie. let's define
\[h_a(t) = \frac{1}{\sqrt{a}} h(t/a)\]
Hence we note that
\[||h_a(t)||^2 = \frac{1}{a} \int_{-\infty}^\infty |h(t/a)|^2 dt = \int_{-\infty}^\infty |h(s)|^2 ds\]
This value is now independent of $a$!\\\\

We now arrive at a new type of transform
\[(W_h f)(\alpha, a) = \frac{1}{\sqrt{a}} \int_{-\infty}^\infty f(t) h(\frac{\alpha - t}{a}) dt\]

This is also called the \textbf{continuous Wavelet transform}. In particular, $h$ is the \textbf{analyzing wavelet}! We want to choose $h$ so that it doesn't pick up any DC components, namely that
\[\int_{-\infty}^\infty h(s) ds = 0\]
For covenience, we also want $h(s)$ to be compactly supported. So we want something like:
\[\includegraphics[width=0.5\textwidth]{Figures/lec8-2}\]
Which looks like the shape of a wavelet!

\newpage
\section{Lecture 9 - 2/13/2023}

\subsection{Continuous Wavelet Transform}
At the end of the last class, we claim up with the following
\begin{definition}
    Let $\psi$ be a real-valued function with $\psi(t)$, $|t|^{1/2} \psi(t)$, and $t \psi(t)$ all integrable with finite energy (ie. square integrable), and that $\widehat{\psi}(0) = 0$ (note this is true if and only if $\int_{-\infty}^\infty \psi(t) dt = 0$). Then the \textbf{continuous wavelet transform (CWT)} of a signal $f(t)$ is
    \[(W_\psi f)(\alpha, a) = \frac{1}{\sqrt{a}} \int_{-\infty}^\infty f(t) \psi(\frac{\alpha - t}{a}) dt,\ \alpha \in \Rbb, a > 0\]
    The landpass filter $\psi$ is called the ``analyzing wavelet".
\end{definition}

\begin{question}
    What are the Time-Frequency properties of this?
\end{question}

In the time domain, we get a window around $t = \alpha$ and radius is given by
\begin{align*}
    \Delta_\psi^2 &= \int_{-\infty}^\infty t^2 \psi(\frac{\alpha - t}{a})^2 dt / \int_{-\infty}^\infty \psi(\frac{\alpha - t}{a})^2 dt\\
    &= a^2 \int_{-\infty}^\infty s^2 \psi(s)^2 ds / \int_{-\infty}^\infty \psi(s)^2 ds \tag*{Let $s = (\alpha - t)/a$}\\
    &= (a \Delta_\psi)^2
\end{align*}
So the time window is $(\alpha - a \Delta_\psi, \alpha + a \Delta_\psi)$.\\

In the frequency domain, we use Parserval's Identity to write $W_\psi f$ in terms of $\widehat{f}(\omega)$. To do this, we need the Fourier Transform of $t \mapsto \psi(\frac{\alpha - t}{a})$. Using techniques in Homework $1$, we get
\[\int_{-\infty}^\infty e^{-i\omega t} \psi(\frac{\alpha - t}{a}) dt = a e^{-i\omega \alpha} \overline{\widehat{\psi}(a\omega)}\]

Hence, by Parserval's Identity, we have that
\[(W_\psi f)(\alpha, a) = \frac{\sqrt{a}}{2\pi} \int_{-\infty}^\infty \widehat{f}(\omega) e^{i \omega \alpha} \widehat{\psi}(a\omega) d\omega \]
This shows that the frequency window is centered on $\omega = 0$ and has radius, with a very similar calculation as before
\begin{align*}
    \widehat{\Delta}^2_{\psi, a} &= \frac{1}{a^2} \widehat{\Delta}_\psi^2
\end{align*}
Hence we have the frequency window on $(-\frac{1}{a} \widehat{\Delta}_\psi^2, + \frac{1}{a} \widehat{\Delta}_\psi^2)$.\\

Our time frequency window looks like:
\[\includegraphics[width=0.5\textwidth]{Figures/lec9-p1.png}\]
with area being $\frac{2}{a} \Delta_\psi (2a \widehat{\Delta}_\psi) = 4 \Delta_\psi \widehat{\Delta}_\psi \geq 2$. We didn't change the area of the window, but we can change the aspect ratio by varying the value of $a$.\\

The flexibility to vary $a$ allows us to change the scale at which resolve time and/or frequency. In a sense, $a$ is similar to a ``zoom factor" on a microscope that allows us to focus in a local feature.

\begin{definition}
    A band-pass filter $\psi(t)$ is a filter with dc component being $0$, ie $\int_{-\infty}^\infty \psi(t) dt = 0$
\end{definition}

\begin{example}
    The Haar Wavelet is
\[\includegraphics[width=0.3\textwidth]{Figures/lec9-p2.png}\]
    This is in some sense the simplest wavelet. It is locally supported, band-pass, etc.
\end{example}


\subsection{Smoothness of Functions}
 The actual value of the filter function doesn't matter, what we care about more is how the function varies over time! This concerns with the ``smoothness" of a given function $f$, ie. we want to know where it is not smooth.

 \begin{question}
     How do we measure smoothness?
 \end{question}

\begin{definition}
    We say $f$ is Lipschitz-continuous at $s$ with exponent $\mu$ if there exists $K < \infty$ and $\mu \in (0, 1)$
    \[|f(s) - f(t)| \leq K |s - t|^\mu \]
    , for $t$ close to $s$. The idea here is that the larger $\mu$ is, the more ``differentiable" the function is.
\end{definition}

\begin{question}
    Can we figure out $s$ and $\mu$ by looking at $\widehat{f}(\omega)$ is?
\end{question}

\begin{theorem}
    If $\widehat{f}$ satisfies $\int_{-\infty}^\infty |\widehat{f}(\omega)| (1 + |\omega|^\mu) d\omega < \infty$, then $f$ is Lipschitz continuous every where with exponent $\mu$.
\end{theorem}

\begin{proof}
    Let's consider $\frac{|f(s) - f(t)|}{|s - t|^\mu}$ and figure out how large $\mu$ can be, so
    \begin{align*}
        f(s) &= \frac{1}{2\pi} \int_{-\infty}^\infty \widehat{f(\omega)} e^{+i\omega s} d\omega\\
        f(t) &= \frac{1}{2\pi} \int_{-\infty}^\infty \widehat{f(\omega)} e^{+i\omega t} d\omega\\
        f(s) - f(t) &= \frac{1}{2\pi} \int_{-\infty}^\infty \widehat{f}(\omega) (e^{i\omega s} - e^{i\omega t}) d\omega
    \end{align*}
    We observe that
    \begin{align*}
        |e^{i\omega s} - e^{i \omega t}| &\leq 2 \tag*{By Triangle's Inequality, this is best bound when $\omega >> 0$} 
    \end{align*}
    On the other hand, we have that
    \begin{align*}
        |e^{i\omega s} - e^{i \omega t}|  &= |[e^{i \omega \theta}]_t^s]|\\
        &= |\int_{t}^s i \omega e^{i \omega \theta} d\theta|\\
        &\leq |\int_t^s d\theta |\omega||\\
        &\leq |\omega| \cdot |s - t| \tag*{Good when $s$ is close to $t$ and $\omega$ is small}
    \end{align*}
    Now, we have that
    \[        \frac{|e^{i\omega s} - e^{i \omega t}|}{|s - t|^\mu} \leq \begin{cases}
            2|s - t|^{-\mu},\\
            |\omega| |s - t|^{1-\mu}
        \end{cases}\]
    If $|s - t| > |\omega|^{-1}$, then $|s - t|^{-1} < |\omega|$, hence $|s - t|^{-\mu} < |\omega|^\mu$.\\\\
    If $|s - t| < |\omega|^{-1}$, then $|s - t|^{1- \mu} < |\omega|^{\mu - 1}$, hence $|\omega| |s - t|^{1-\mu} < |\omega|^\mu$, thus we always have that
    \[ \frac{|e^{i\omega s} - e^{i \omega t}|}{|s - t|^\mu} \leq  C |\omega|^\mu\]
    for some constant $C$, hence we have that
    \begin{align*}
    \frac{|f(s) - f(t)|}{|s - t|^\mu} &\leq \frac{1}{2\pi} \int_{-\infty}^\infty |\widehat{f}(\omega)| \cdot \frac{|e^{i\omega s} - e^{i\omega t}|}{|s - t|^\mu} d\omega\\
    &\leq \frac{C}{2\pi} \int_{-\infty}^\infty |\widehat{f}(\omega)| (1 + |\omega|^\mu) d\omega\\
    &< \infty
    \end{align*}
\end{proof}

Good news - looking at the decay of $|\widehat{f}(\omega)|$ as $|\omega| \to \infty$ tells us about the smoothness of $f$.\\\\

Bad news - this tells us nothing about the local smoothness, ie. how doe sthe smooth vary with respect to position! ($\mu$ is the lower bound on the smoothness of all the points).\\\\

Recall that for $t \mapsto f(t - \alpha)$, the Fourier transform is $\omega \mapsto e^{i \alpha \omega} \widehat{f}(\omega)$, but the fourier transform DOES NOT translate anything, so its modulus is the same as $|\widehat{f}(\omega)|$. Hence no matter how we try to tweak around the theorem, we won't have the local information.

\newpage
\section{Lecture 10 - 2/15/2023}

Recall that we have established, last time, that:
\begin{itemize}
    \item Locally Lipschitz - $f$ is locally Lipschistz at $\alpha$ with exponent $\mu$ if
    \[|f(\alpha) - f(t)| \leq K|\alpha - t|^\mu\]
    where $\mu$ indicates/measures how smooth $f$ is at $\alpha$
    \item Can we deduce $\mu$ without looking directly at $f(t)$ and $t \in \Rbb$
    \item We proved last lecture that:
\begin{theorem}
    If $\widehat{f}$ satisfies
    \[\int_{-\infty}^\infty |\widehat{f}(\omega)| (1 + |\omega|^\mu) d\omega < \infty\]
    then $f$ is globally Lipschitz with exponent $\mu$.
\end{theorem}
    \item As a corollary, $f(t) = \frac{1}{2\pi} \int_{-\infty}^\infty e^{i\omega t} \widehat{f}(\omega) d\omega$ implies that
    \[|f(t)| \leq \frac{1}{2\pi} \int_{-\infty}^\infty |\widehat{f}(\omega)| d\omega < \infty\]
    So the $1 +$``something" means $f$ is a well-defined behavior. However, this theorem only tells us the \textbf{global $\mu$}, but some parts of $f$ might do better than this global $\mu$. This means we have to analyze the behaviors of $f$ more locally.
\end{itemize}

\begin{question}
    Can we find a better way to identify $\mu$ and hopefully how smooth this is at $\alpha$?
\end{question}

Consider the Lipschitz condition, 
\[|f(\alpha) - f(t)| \leq K |\alpha - t|^\mu\]
Let's write $t = \alpha - as$ for some $a > 0$ and $s$, then we have that
\[|f(\alpha) - f(\alpha - as)| \leq K a^\mu |s|^\mu\]
Suppose we scale $a \mapsto a/2$, then the $K a^\mu |s|^\mu$ becomes $K a^\mu |s|^\mu/2^\mu$. This means that we can interpret Lipschitz as a scaling relation where $\mu$ measures the relative change on $f$ when the arguments get closer (recall $0 < \mu < 1$).\\

This suggests that by varying ``$a$", we should be able to detect the value of $\mu$ when we examine the change in $f$.\\

Let's recall the continuous wavelet transform:
\[(W_h f)(\alpha, a) = \frac{1}{\sqrt{a}} \int_{-\infty}^\infty f(t) h(\frac{\alpha - t}{a}) dt\]
We can interpret $a$ as a ``zoom" or ``scale" factor, and $W_h f$ is the response to the scaling. We claim it is plausible that we can use the CWT to identity $\mu$!\\

We will now make this assertion more precise:
\begin{align*}
    (W_h f)(\alpha, a) &= \frac{1}{\sqrt{a}} \int_{-\infty}^\infty f(s) h(\frac{\alpha - s}{a}) ds\\
    &= \sqrt{a} \int_{-\infty}^\infty f(\alpha - at) h(t) dt \tag*{Let $t = \frac{\alpha - s}{a}$}
\end{align*}
When we change $a \mapsto a/2$, is there a $\mu$-dependent response above? There's no apparent behavior that depends on $\mu$, the scalar doesn't matter, $f(\alpha - at)$ converges to $f(\alpha)$, but this has nothing to do with $\mu$.\\

But, we recall in the Lipschitz condition, we are really comparing $|f(\alpha) - f(\alpha - at)|$, can we introduce a $f(\alpha)$ in here? Indeed, let we assume that $\int_{-\infty}^\infty h(t) dt = 0$, then
\[(W_h f)(\alpha, a) = \sqrt{a} \int_{-\infty}^\infty [f(\alpha - at) - f(\alpha) + f(\alpha)] h(t) dt = \sqrt{a} \int_{-\infty}^\infty [f(\alpha - at) - f(\alpha)] h(t) dt + 0  \]

Estimate using Lipschitz, 
\begin{align*}
    |W_h f(\alpha, a)| &\leq \sqrt{a} \int_{-\infty}^\infty |f(\alpha - at) - f(\alpha)| |h(t)| dt\\
    &\leq \sqrt{a} \int_{-\infty}^\infty K|at|^\mu |h(t)|\ dt \tag*{Lipschitz Continuity}\\
    &= a^{1/2 + \mu} K \int_{-\infty}^\infty |t|^\mu |h(t)|\ dt
\end{align*}
This suggests that we can detect $\mu$ locally by seeing how the Wavelet transform behaves with ``$a$"!

\begin{example}
    Consider the function $f(t) = |t|$, this is not smooth at $t = 0$. We see that
    \[|f(t) - f(s)| = ||t| - |s|| \leq |t - s|^\mu\]
    The highest value of $\mu$ should be $1$! Because this is just the reverse triangle inequality. Let's consider
    \begin{enumerate}
        \item $h(t) = e^{-t^2/2}$, then
        \begin{align*}
            (W_h f)(0, a) &= \sqrt{a} \int_{-\infty}^\infty |\alpha - at| e^{-t^2/2} dt
        \end{align*}
        There's no $\mu$-dependent response here... This is no good because $h(t)$ is not an analyzing wavelet! ($\int_{-\infty}^\infty h(t) dt \neq 0$)
        \item Let $h(t)$ be the Haar wavelet:
        \[\includegraphics[width=0.3\textwidth]{Figures/lec10-p1.png}\]
        and we have that
        \begin{align*}
            (W_h f)(0, a) &= \sqrt{a} \int_{-\infty}^\infty h(t) |at| dt\\
            &= a^{3/2} \int_{-\infty}^\infty |t| h(t) dt
        \end{align*}
        This varies as $a^{3/2} = a^{1/2 + 1}$, BUT $\int_{-\infty}^\infty |t| h(t) dt = 0$, so this actually doesn't change as you change $a$. Hence this is not very useful. But we can still look at $(W_h f)(\alpha, a)$ with $\alpha \approx 0$.
        \item Derivative Gaussian Wavelet - this is $h(t) = - \frac{d}{dt} e^{-t^2/2} = t e^{-t^2/2}$, which graphically:
        \[\includegraphics[width=0.3\textwidth]{Figures/lec10-p2.png}\]
        Unfortunately, this has the same problem as
        \[(W_h f)(0, a) = a^{3/2} \cdot 0 = 0\]
        \item Let's consider $h(t) = \frac{d^2}{dt^2}(e^{-t^2/2}) = (1 - t^2) e^{-t^2/2}$, this is called the ``Mexican Hat Wavelet", whose picture is given above.
        We have that
        \[(W_h f)(0, a) = \sqrt{a} \int_{-\infty}^\infty |t| h(t) dt = -2 a^{3/2}\]
        THis does respond to $a^{1/2 + \mu}$! We also observe that
        \[(W_h f)(\alpha, a) = -2 a^{3/2} e^{-\alpha^2/(2a^2)}\]
        Plotting $\alpha$ against $a > 0$, recall that $e^{-\alpha^2 /2a^2} \approx 0$ when $|\alpha/\sqrt{2a}| \geq 6$
        \[\includegraphics[width=0.3\textwidth]{Figures/lec10-p4.png}\]
        This points to where the singularity is, the rate of increase around the singularity is how bad the singularity is.
    \end{enumerate}
\end{example}

\newpage
\section{Lecture 11 - 2/17/2023}

\subsection{Higher Moments}
We recall that last lecture, we obtained the following two equivalent formulation of the Continuous Wavelet Transform:
\begin{align*}
    (W_h f)(\alpha, a) &= \frac{1}{\sqrt{a}} \int_{-\infty}^\infty f(t) h(\frac{\alpha - t}{a}) dt\\
    &= \sqrt{a} \int_{-\infty}^\infty f(\alpha - as) h(s) ds \tag*{Let $s = \frac{\alpha - t}{a}$}
\end{align*}

\begin{question}
    What key property of the Mexican hat makes it a better filter for $f(s) = |s|$?
\end{question}

Indeed, let $h(t) = (1 - t^2) e^{-t^2/2}$, we first note 
\[\widehat{h}(0) = \int_{-\infty}^\infty h(s) ds = 0\]
Thus, we have that
\begin{align*}
    (W_h f)(\alpha, s) &= \sqrt{a} \int_{-\infty}^\infty f(\alpha - as) h(s) ds\\
    &= \sqrt{a} \int_{-\infty}^\infty [f(\alpha - as) - f(\alpha)] h(s) ds
\end{align*}
This allows us to use the Lipschitz property!

\begin{question}
    Can we look at higher degrees of smoothness? (ie. when the derivatives are also smooth)
\end{question}

Indeed, we can think of the condition $\int_{-\infty}^\infty h(s) ds = 0$ as the fact that the zeroth moment of $h(s)$ is $0$. If $h(s)$ vanishes on higher moments too, we can write
\[(W_h f)(\alpha, a) = \sqrt{a} \int_{-\infty}^\infty [f(a - \alpha s) - \text{linear polynomial}] h(s) ds\]

What is the term $f(a - \alpha s) - \text{linear polynomial}$? Well, we can write 
\[f(s) = f(0) + s f'(0) + ...\]
using Taylor expansions. For the zero-th moment, we have that
\[f(s) - f(0) = s f'(0) + ... \]
In this case, we have filtered out the constant term (ie. the DC component). For higher order moments, we want to filter out more terms. To do this, we want the first $n$ orders of $h(s)$ to be $0$, that is
\[\int_{-\infty}^\infty s^k h(s) ds = 0, 0 \leq k \leq n\]

\begin{example}
    If we look back at the Mexican Hat example again, we observe that
    \[\int_{-\infty}^\infty s (1 - s^2) e^{-s^2/2} ds = 0\]
    Hence, the first order moment of the filter is also $0$.\\\\
    This means that $(W_h f)(\alpha, a) \approx 0$ at non-zero values too!\\\\
    If we recall the examples we used last time, the Gaussian filter $h(t) = e^{-t^2/2}$ fails because its zeroth moment is non-zero. The Haar wavelet fails because its first moment is non-zero, hence it is sensitive to derivatives. The first moment of the derivative Gaussian is similarly also non-zero.
\end{example}

\subsection{Criterion for Selecting a Wavelet}

In general we want out filter $h(s)$ to satisfy:
\begin{itemize}
    \item $\int_{-\infty}^\infty s^m h(s) ds = 0$ for $m = 0, 1, ..., M$ where $M$ is taken to be necessary. This implies that $h(s)$ should also be oscillating.
    \item $h(s)$ has rapid decay or compactly supported.
\end{itemize}

Note that sometimes we do not want to make $M$ too big. For example,
\[\frac{d^{1066}}{d t^{1066}} (e^{-t^2/2})\]
is a good filter moment wise, but it is really complicated to work with because we will be dealing with $e^{-t^2/2} \times \text{polynomial of degree $1066$}$.

Thus, we have that
\[(W_h f)(\alpha, a) = \frac{1}{\sqrt{a}} \int_{-\infty}^\infty f(t) h(\frac{t - \alpha}{a}) dt\]
Generally, this integral is not directly solvable, so we would want to obtain a numerical approximation, for fixed $\alpha, a$, we have
\[(W_h f)(\alpha, a) \approx \frac{1}{\sqrt{a}} \sum_{n \in \Zbb} f(n) h(\frac{n - \alpha}{a})\]
where $f(n)$ is thought of as the ``digital representation" of $f$. This is because computers can't represent functions analytically, so we instead take at each digit.

\begin{definition}
    Let $\{f(t): t \in \Rbb\}$ be a signal, we call
    \[\{f(n): n \in \Zbb\}\]
    the digital representation of the signal.
\end{definition}

\newpage

\section{Lecture 12 - 2/22/2023}

\subsection{Band-Limited Signals and Shannon Sampling Theorem}

Consider the following problem:

\begin{question}
We have a digital phone exchange where calls are transmitted as digital signals. $\{f(t): t \in \Rbb\}$ is thought of as an analog signal, no information is really lost here, but now we only have packets of the signal (thought of as digital), so we only know the signal at integer values. In other words, we only have the information $\{f(m): m \in \Zbb\}$.\\

The question is - how often do we need to sample the signal in order to get a perfect reconstruction at the other end? (... why can we even consider getting a perfect reconstruction? If there's no constraint on the function other than continuity, you can find infinity many functions fitting through the digital signal.) 
\end{question}

At a mathematical level, the question makes no sense (because frequency can be arbitrarily high). However, in practice, we do have a constraint on the signal because we are thinking about this in the context of a ``phone exchange", so we are only interested in certain frequency ranges. It is then natural to consider signals such that the frequency is limited:
\[\widehat{f}(\omega) = 0 \text{ if $|\omega| > \Omega$}\]
for some $\Omega \geq 0$.\\

Let's now reformulate our question:
\begin{question}[Shannon's Problem]
    Suppose $f$ is band-limited by $\Omega$, how often do we need to sample $f$ in order to reconstruct $f$ from the digital samples?
\end{question}

\subsection{Poisson Summation Formula}
In order to answer this question, we derive a useful mathematical tool - the Poisson Summation Formula.\\

Suppose that $\widehat{f}(\omega)$ satisfies a decay condition (this holds for all band-limited signals but we are considering a more general case here):
\[|\widehat{f}(\omega)| \leq C (1 + |\omega|)^{-\alpha}\ \text{ for some $\alpha > 1$}\ (*)\]
(Note that the condition for Lipschitz $\alpha$ we proved implies this, but the converse is not true).\\\\

To see that band-limited signals satisfy this, we can choose $C = (1 + |\Omega|)^{\alpha} \cdot \max_{|\xi| \leq \Omega} |\widehat{f}(\xi)|$, hence
\[|\widehat{f}(\omega)| \leq \frac{(1 + |\Omega|)^{\alpha}}{(1 + |\omega|)^{\alpha}} \max_{|\xi| \leq \Omega} |\widehat{f}(\xi)|\]

In particular, $(*)$ means that
\[\int_{-\infty}^\infty |\widehat{f}(\omega)| d\omega \leq C \int_{-\infty}^\infty \frac{d\omega}{(1 + |\omega|)^\alpha} < \infty\]

Hence we have that
\[|\int_{-\infty}^\infty e^{i\omega t} \widehat{f}(\omega) d\omega| \leq \int_{-\infty}^\infty |\widehat{f}(\omega)| d\omega < \infty\]
Thus, \underline{Fourier series is well-defined and $f$ is well-defined}.\\

\begin{definition}
Let $p > 0$ be given, and define a periodisation of $\widehat{f}$ as follows:
\begin{align*}
    \widehat{f}_{per}(\omega) &= \sum_{k \in \Zbb} \widehat{f}(w + kp)
\end{align*}
Then, $\widehat{f}_{per}(\omega)$ is clearly periodic with period $p$. 
\end{definition}

\begin{proposition}
    $\widehat{f}_{per}(\omega)$ is well-defined for all $\omega \in \Rbb$.
\end{proposition}

\begin{proof}
    To see that the sum converges, we only need to consider $\widehat{f}_{per}(\omega)$ for $\omega \in [0, p]$ and note that
\begin{align*}
    \int_0^p |\widehat{f}_{per}(\omega)| d\omega &= \int_0^p |\sum_{k \in \Zbb} \widehat{f}(\omega + kp)| d\omega\\
    &\leq \sum_{k \in \Zbb} \int_0^p |\widehat{f}(\omega + kp)| d\omega\\
    &= \sum_{k \in \Zbb} \int_{kp}^{kp +p} |\widehat{f}(\xi)| d\xi \tag*{Let $\xi = \omega + kp$}\\
    &= \int_{-\infty}^\infty |\widehat{f}(\xi)| d\xi\\
    &< \infty
\end{align*}
\end{proof}

In particular, $\widehat{f}_{per}(\omega)$ has a discrete Fourier series.
\[\widehat{f}_{per}(\omega) = \sum_{n \in \Zbb} c_n e^{-2\pi i \omega n/p},\ \omega \in [0, p]\]
where
\[c_n = \int_0^p \frac{1}{p} \widehat{f}_{per}(\omega) e^{2\pi \omega n/p} d\omega\]
Computing $c_n$ explicitly:
\begin{align*}
    c_n &= \int_0^p \frac{1}{p} \widehat{f}_{per}(\omega) e^{2\pi \omega n/p} d\omega\\
    &= \frac{1}{p} \sum_{k \in \Zbb} \int_0^p \widehat{f}(\omega + kp) e^{2\pi i \omega n/p} d\omega\\
    &= \frac{1}{p} \sum_{k \in \Zbb} \int_{kp}^{kp+p} \widehat{f}(\xi) e^{2\pi i(\xi - kp)n/p} d\xi \tag*{Let $\xi = \omega + kp$}\\
    &= \frac{1}{p} \sum_{k \in \Zbb} \int_{kp}^{kp+p} \widehat{f}(\xi) e^{2\pi i(\xi)n/p}\cdot 1 d\xi\\
    &= \frac{1}{p} \sum_{k \in \Zbb} \int_{kp}^{kp+p} \widehat{f}(\xi) e^{2\pi i(\omega)n/p} d\omega\\
    &= \frac{1}{p} \int_{-\infty}^\infty \widehat{f}(\omega) e^{2\pi i \omega n/p} d\omega\\
    &= (\frac{2\pi}{p}) (\frac{1}{2\pi}) \int_{-\infty}^\infty \widehat{f}(\omega) e^{2\pi i \omega n/p} d\omega\\
    &= \frac{2\pi}{p} f(\frac{2\pi n}{p}) \tag*{By Fourier Inversion Formula}
\end{align*}

Hence, we obtain the Possion Summation Formula:
\begin{theorem}
Let $p > 0$ be fixed and $\widehat{f}$ be a function that satisfies then aforementioned decay condition, then
   $$\widehat{f}_{per}(\omega) = \frac{2\pi}{p} \sum_{n \in \Zbb} f(\frac{2\pi n}{p}) e^{-2\pi i \omega n/p}$$
for all $\omega \in [0, p]$
\end{theorem}

This is really useful for proving various identities:
\begin{corollary}[Basel Problem]
    $\sum_{n = 1}^\infty \frac{1}{n^2} = \frac{\pi^2}{6}$
\end{corollary}

\begin{proof}
    Exercise!
\end{proof}

\begin{remark}
    The closed form value of $\sum_{n = 1}^\infty \frac{1}{n^3}$ is not known! It's interesting to investigate why you can't use Possion Summation Formula to solve this.
\end{remark}

\newpage
\section{Lecture 13 - 2/24/2023}

\subsection{Proof of the Shannon Sampling Theorem}
Recall in the last lecture, we have written out the \textbf{Poisson Summation Formula}:
\[\sum_{k \in \Zbb} \widehat{f}(\omega + pk) = \frac{2\pi}{p} \sum_{n \in \Zbb} f(\frac{2\pi n}{p}) e^{-2\pi i n \omega/p}\]
We'll use this to answer the sampling question, but before that - let's rethink about what we want to do:

\begin{question}
    Suppose we have $f$ sampled at discrete points $f(0), f(\pm h), f(\pm 2h), ...$, and that $f$ is band limited with bandwidth $\Omega$. How small should $h$ be (depending on $\Omega$) such that we can reconstruct $f$ exactly, ie. we want a formula of the form
    \[f(t) = \sum_{n \in \Zbb} f(nh) \cdot [\text{some correcting term here}]\]
    In particular, our correcting term should probably depend on $t, n, $ and $h$, hence
    \[f(t) = \sum_{n \in \Zbb} f(nh) \phi(t; n, h) \]
    Then our question really is - how should we construct $\phi(t; n, h)$?
\end{question}

Here are some properties of $\phi$ that we would expect:
\begin{itemize}
    \item $\phi(mh; m, h) = 1$ and $\phi(mh; n, h) = 0$ if $m \neq n$
    \item What is $\phi(t - h; m, h)$? Well,
    \[f(t - h) = \sum_n f(n, h) \phi(t - h; n, h) = \sum_n f(n h + h) \phi(t; n, h)\]
    They really have the same coefficients shifted by a spot, so we have that
    \[\phi(t - h; n, h) = \phi(t; n+1, h)\]
    More generally,
    \[\phi(t - mh; n, h) = \phi(t; n + m, h)\]
    This means that the structure of $\phi$ is very simple. Choose $n = 0$, then
    \[\phi(t - mh; 0, h) = \phi(t; m, h) \forall t\]
    Let's call $\varphi_0(t, h) = \phi(t; 0, h)$, then $\phi$ is really just determined by what $\varphi_0$ is!
    \item We conclude that $\phi(t; m, h)$ is a translate of $\varphi_0(t; h)$
    \item $\varphi_0(t; h)$ has to be an even function with respect to $t$ which decays as $|t| \to \infty$
    \item What's the last variable we haven't messed with? It's $h$! We also expect a ``scaling property" in terms of how we'd change $h$,
    \[\varphi_0(t; h) \propto \varphi_0(\frac{t}{h})\]
\end{itemize}

A pretty good suspect of a function that satisfies this is the \textbf{sink function}!

\begin{theorem}[Shannon Sampling Theorem]
    Suppose $\widehat{f}(\omega) = 0$ for all $|\omega| > \Omega$ with $f \in L_2(\Rbb)$. If $0 < a \leq \frac{\pi}{\Omega}$, then $f(t)$ is given by
    \[f(t) = \sum_{k \in \Zbb} f(ka) \phi_s(\frac{t}{a} - k)\]
    where $\phi_s(t) = \frac{\sin \pi t }{\pi t}$ is the ``Shannon Sampling Function".
\end{theorem}

\begin{proof}
    Let's start with the Poisson Summation Formula:
    \[\sum_{k \in \Zbb} \widehat{f}(\omega + pk) = \frac{2\pi}{p} \sum_{n \in \Zbb} f(\frac{2\pi n}{p}) e^{-2\pi i n \omega/p}\]
    Let's multiply both sides by
    \[\sum_{k \in \Zbb} \widehat{f}(\omega + pk) e^{2\pi i \omega t/p} = \frac{2\pi}{p} \sum_{n \in \Zbb} f(\frac{2\pi n}{p}) e^{2\pi i\omega (t - n)/p}\]
    Let's integrate this expression on both sides with respect to $\omega$:
    \begin{align*}
        \sum_{k \in \Zbb} \int_{-p/2}^{p/2} \widehat{f}(\omega + pk) e^{2\pi i \omega t/p} d\omega &= \frac{2\pi}{p} \sum_{n \in \Zbb} f(\frac{2\pi n}{p}) \int_{-p/2}^{p/2} e^{2\pi i\omega (t - n)/p} d\omega
    \end{align*}
    The integral on the RHS gives us
    \begin{align*}
        \int_{-p/2}^{p/2} e^{2\pi i\omega (t - n)/p} d\omega &= \frac{p}{2\pi i (t-n)}[e^{2\pi i \omega (t - n)/p}]_{-p/2}^{p/2}\\
        &= \frac{p}{2\pi i (t-n)} [\frac{2i \sin(\pi(t - n))}{1}]\\
        &= p \frac{\sin \pi(t - n)}{\pi(t - n)}\\
        &= p \phi_s(t - n)
    \end{align*}
    The integral on the LHS should give $f(t)$! But we need $p/2$ to be sufficiently large so that $p/2 > \Omega$ so that we can get the entire integral on the LHS (since $\widehat{f}(\omega)$ is band-limited). To do this, we choose $a = \frac{2\pi}{p}$. The condition that $p/2 > \Omega$ is then equivalent to $\frac{\pi}{a} > \Omega$.\\\\
    \begin{align*}
        LHS &= \sum_{k \in \Zbb} \int_{-p/2}^{p/2} \widehat{f}(w + kp) e^{2\pi i \omega t/p} d\omega\\
        &= \int_{-p/2}^{p/2} \widehat{f}(\omega) e^{2\pi i \omega t/p} \tag*{If $p/2 > \Omega$, this is because all other values of $k$ have $\widehat{f}(w + kp) = 0$}\\
        &= \int_{-\infty}^\infty \widehat{f}(\omega) e^{2\pi i \omega t/p} d\omega \tag*{Since $p/2 > \Omega$, integrating over the support}\\
        &= 2\pi f(\frac{2\pi t}{p})
    \end{align*}
    Hence our equation becomes
    \[2\pi f(\frac{2\pi t}{p}) = 2\pi \sum_{n \in \Zbb} f(\frac{2\pi n}{p}) \phi_s(t - n)\]
    for $p > 2 \Omega$.\\\\
    So we have that
    \[f(\frac{2\pi t}{p}) = \sum_{n \in \Zbb} f(\frac{2\pi n}{p}) \phi_s(t - n)\]
    Write $\frac{2\pi}{p} = a$, $p > 2 \Omega$ is the same as $a < \frac{\pi}{\Omega}$, hence we have that
    \[f(at) = \sum_{n \in \Zbb} f(an) \phi_s(t - n)\]
    A change of variables gives the desired equation given by the Theorem and finishes the proof.
\end{proof}

\newpage
\section{Lecture 14 - 2/27/2023}

\subsection{Oversampling}

Consider the Shannon-Whittaker Formula, we have
\[f(t) = \sum_{n \in \Zbb} f(an) \phi_s(\frac{t}{a} - n)\]
provided that:
\begin{itemize}
    \item $f$ has bandwidth $\Omega$ and is in $L^2(\Rbb)$
    \item $a \leq \pi/\Omega$
    \item $\phi_s(t) = \frac{\sin \pi t}{\pi t}$ is the ``Shannon Sampling Function"
\end{itemize}

There are some problems with this in practical applications:
\begin{itemize}
    \item This formula is an infinite sum
    \item If we want to truncate the equation and approximate $f$, we get a slowly convergent series because
    \[\phi_s(t) \sim \frac{1}{t} \text{ as } t \to \infty\]
\end{itemize}

\begin{example}
    Consider the sum
    \[S = \sum_{n = 0}^\infty \frac{(-1)^n}{n+1}\]
    How many terms are needed to get an accuracy $10^{-3}$? You need about $1000$ terms! However, we note that
    \[S = (\frac{1}{1} - \frac{1}{2}) + (\frac{1}{3} - \frac{1}{4}) + ...\]
    So in general we have that
    \[\frac{1}{2r - 1} - \frac{1}{2r} = \frac{+1}{2r(2r - 1)}\]
    So we have that
    \[S = \sum_{r = 1}^\infty \frac{1}{2r(2r - 1)}\]
    How many terms do we need to get to $10^{-3}$? It's roughly $\sqrt{1000} \approx 30$! There are a lot of series where you can tweak to make it converge faster.
\end{example}

\begin{question}
    Can we rewrite the Shannon-Whittaker Formula to make the sum converge faster? In other words, we seek a formula of the same form
    \[f(t) = \sum_{n \in \Zbb} f(an) \phi(t/a - n),\ a \leq \pi/\Omega\ (\dagger)\]
    that is exact when $f$ has bandwidth $\Omega$ and such that $\phi(t)$ decays more rapidly as $t \to \infty$ than $\phi_s(t)$.
\end{question}

Let's start by trying to construct a counter-example:

\begin{example}
    Let $\Omega > 0$ be given, then choose
    \[f(t) = \phi_s(t/a) = \frac{\sin \pi t/a}{\pi t/a}\]
    for some fixed $a$. Then $f$ has bandwidth $\pi/a$. We want to choose $a$ such that $\pi/a \leq \Omega$ for sanity.\\\\
    We also need $a \leq \pi/\Omega$ because that's an additional constraint in our original question.\\\\
    Why don't we just choose $a = \pi/\Omega$, so $f$ has bandwidth $\Omega$. Plugging this into $(\dagger)$ gives us
    \[\phi_s(t/a) = \sum_{n \in \Zbb} \phi_s(an) \phi(t/a - n) = \sum_{n \in \Zbb} \delta_{n, 0} \phi(t/a - n) = \phi(t/a) \]
    where $\delta_{n, 0}$ is the Kronecker delta symbol that is $0$ on all $n \neq 0$ and $1$ on $n = 0$.\\\\
    Hence, we have that $\phi = \phi_s$, so we have a counter-example?
\end{example}

But ... there's some problem with the counter-example we just gave. If we choose $a = \pi/\Omega$, we always get our conclusion above. But what if we choose $a < \pi/\Omega$? Then the example above is no longer a counter-example.\\

If $a < \pi/\Omega$, then we have actually over-parametrized the system, there are redundancies in the formula because we have more data than we need. If $a = \pi/\Omega$, the formula has no redundancy! The extra redundancy in $a < \pi/\Omega$ let's us exploit new linear dependencies to achieve a faster decay.\\

The process of $a < \pi/\Omega$ is known as \textbf{oversampling}. The hope is that - by oversampling, we could get a formula that converges faster.\\

Let's then choose $a < \pi/\Omega$ ($\pi/\Omega$ is sometimes called the \textbf{Nyquist Rate}), so there is flexibility in the choice of $\phi$.\\

Let's rethink about our proof of Shannon's Sampling Theorem, let's consider the Poisson SUmmation Formula:
\[\sum_{k \in \Zbb} \widehat{f}(w + kp) = \frac{2\pi}{p} \sum_{n \in \Zbb} f(2\pi n/p) e^{2\pi i n \omega/p}\]

Now let's suppose that $f$ has bandwidth $\Omega$, we should consider $\omega \in (-\Omega, \Omega)$ as this is periodic.\\

If $k \geq 1$, then $w + kp \geq w + p \geq -\Omega + p$. If $p$ is large enough, we can choose $p \geq 2\Omega$ such that
\[w + kp \geq -\Omega + p \geq \Omega\]

Thus, if $p \geq 2 \Omega$ and $k \geq 1$, then
\[\widehat{f}(w + kp) \equiv 0\]

On the other hand, if $k \leq -1$, then $w + kp \leq w - p \leq \Omega - p \leq -\Omega$ if $p \geq 2 \Omega$.\\

Hence, if $p \geq 2\Omega$, the Poisson Summation Formula becomes
\[\widehat{f}(w) = \frac{2\pi}{p} \sum_{n \in \Zbb} f(\frac{2\pi n}{p}) e^{-2\pi i n \omega/p}\]
for $\omega \in (-\Omega, \Omega)$.\\

Let's write $2\pi/a = p \geq 2\Omega$, then our condition is that $a \leq \pi/\Omega$.\\\\

Let's choose $\widehat{g}$ such that $\widehat{g}(\omega) = 1$ for all $|\omega| \leq \Omega$, then we have that
\[\widehat{f}(\omega) = \widehat{f}(\omega) \widehat{g}(\omega) = a \sum_{n \in \Zbb} f(a_n) e^{-i a \omega n} \widehat{g}(\omega)\]

Taking the Inverse Fourier Transform on both sides gives
\begin{align*}
    f(t) &= \frac{1}{2\pi} \int_{-\infty}^\infty e^{i\omega t} \widehat{f}(\omega) d\omega \\
    &= a \sum_{n \in \Zbb} f(an) [\frac{1}{2\pi} \int_{-\infty}^\infty \widehat{g}(\omega) e^{i \omega (t - an)} d\omega]\\
    &= a \sum_{n \in \Zbb} f(an) g(t - an)
\end{align*}
for all $f$ with bandwidth less than $\Omega$ and that $a \leq \pi/\Omega$.\\

Thus, we can choose $\phi(t/a) = g(t)$ in $\dagger$.

\begin{remark}
    Note that $\widehat{g}$ can essentially be any function outside the interval $[-\Omega, \Omega]$, so the only constraint is that is has to be constant within the interval.\\\\
    But if we are not over-sampling, ie. when $a = \pi/\Omega$, our previous counter-example kicks in again.
\end{remark}

\newpage
\section{Lecture 15 - 3/1/2023}

Recall last lecture, we have proven our new theorem:
\begin{theorem}
    Let $\Omega > 0$, 
    \[
    \widehat{g}(\omega) = \begin{cases}
        1,\ |\omega| \leq \Omega\\
        \text{anything}, |\omega| > \Omega
    \end{cases}
    \]
    If $f$ has bandiwdth $\leq$ $\Omega$, then
    \[f(t) = a \sum_{n \in \Zbb} f(an) g(t - an)\]
    for all $a \leq \pi/\Omega$.
\end{theorem}

This gives flexibility for us in the choice of $g$. We will list some notable applications:

\begin{example}[Recovering one Lost Signal]
    Choose
    \[\widehat{g}(\omega) = \begin{cases}
        1,\ |\omega| \leq \Omega\\
        0,\ |\omega| > \Omega
    \end{cases}\]
    Then we have that
    \[g(t) = \frac{1}{2\pi} \int_{-\Omega}^\Omega e^{i\omega t} d\omega = \frac{\sin \Omega t}{\pi t}\]
    Then we have that
    \[a g(t) = \frac{a \Omega}{\pi} \frac{\sin \Omega t}{\Omega t} = \frac{a \Omega}{\pi} \Phi_s(\frac{\Omega}{\pi} t)\]
    where $\Phi_s(t) = \frac{\sin \pi t}{\pi t}$ is the usual Shannon Sampling Function. Applying the Theorem previously, we have that
    \[f(t) = \frac{a \Omega}{\pi} \sum_{n \in \Zbb} f(an) \Phi_s(\frac{\Omega}{\pi}(t - an)),\ a \leq \pi/\Omega\]
    When $a \Omega /\pi = 1$, then this reduces to $\frac{\Omega}{\pi} = 1/a$, which returns the Shannon-Whittaker formula. This is what we expect, and we get nothing new by choosing $a = \frac{\Omega}{\pi}$. This is because we are not oversampling.\\\\
    Suppose we instead over-sample, ie. have that $a < \frac{\pi}{\Omega}$. We define the sampling rate $\beta$ as
    \[\beta \coloneqq \frac{\pi}{\Omega a} > 1\]
    (ie. if $\beta = 2$, we are sampling twice as often, etc.) Using the Theorem as before, formula then becomes,
    \[f(t) = \frac{1}{\beta} \sum_{n \in \Zbb} f(\frac{\pi n}{\beta \Omega}) \Phi_s(\frac{\Omega t}{\pi} - \frac{1}{\beta})\]
    When $t = 0$, we have
    \[f(0) = \frac{1}{\beta} \sum_{n \in \Zbb} f(\frac{\pi n}{\beta \Omega}) \Phi_s(\frac{n}{\beta})\]
    Observe that $f(0)$ appears on both sides. Look at the coefficients of $f(0)$ on the RHS, ie.
    \[\frac{1}{\beta} \Phi_s(0) f(0) = \frac{1}{\beta}(1) f(0)\]
    But since $\beta > 1$. we kmow that $\frac{1}{\beta} f(0) \neq f(0)$, so we have that
    \[f(0) = \frac{\beta}{\beta - 1} \sum_{n \in \Zbb - \{0\}} f(\frac{\pi n}{\beta \Omega}) \Phi_s(\frac{n}{\beta})\]
    Suppose we transmit $\{f(\frac{\pi n}{\beta \Omega}: n \in \Zbb\}$ but the value at $n = 0$ is lost, we can actually recover it by oversampling.
\end{example}

More generally, if we replace $f(t)$ by $f(t + \frac{\pi m}{\beta \Omega})$, then we get
\[f(\frac{m \pi}{\beta \Omega}) = \frac{\beta}{\beta - 1} \sum_{n \neq 0} f(\frac{\pi}{\beta \Omega}(m + n)) \Phi_s(\frac{n}{\beta}) \]
Let $\ell = m + n$, we have this denoising filter
\[f(\frac{m \pi}{\beta n}) = \frac{\beta}{\beta - 1} \sum_{\ell \neq m} f(\frac{\pi \ell}{\beta \Omega}) \Phi_s(\frac{\ell - m}{\beta}),\ m \in \Zbb\]
These relations are the basis for ``digital remastering" or ``noise reduction" in signal.\\

Let's examine another application. Despite being a big deal, these formulae are not usually used in the above form. This is because this is still an infinite sum that doesn't decay really fast.\\

Let's try to use the flexibility in the choice of $\widehat{g}$ to obtain a new (faster) converging formula.\\

The inverse Fourier Transform of $g$ needs to decay faster! So we want to make $\widehat{g}$ decay slower (this is what the time-frequency inequality tells us). Let's try to extend the support from $[-\Omega, \Omega]$ to $[-\beta \Omega, \beta \Omega]$ for $\beta > 1$:
\[\widehat{g}(\omega) = \begin{cases}
    1,\ |\omega| \leq \Omega\\
    \text{linear},\ \Omega \leq |\omega| \leq |\beta \Omega|\\
    0,\ \text{otherwise}
\end{cases}\]

Converting $\widehat{g}$ to $g$ gives:
\[g(t) = \frac{\cos \Omega t - \cos \beta \Omega t}{\pi t^2 (\beta - 1) \Omega}\]

This converges a lot faster, we observe that
\begin{itemize}
    \item L'Hopital's Rule gives us 
 \[\lim_{\beta \to 1} g(t) = \frac{\sin \Omega t}{\pi t}\ (\text{as expected})\]   
    \item We also have that
\[g(t) \sim \frac{1}{t^2} \text{ as } |t| \to \infty \]
\item $\lim_{t \to 0} g(t) \frac{1}{2} \Omega (\beta + 1) \neq 1$. The fact that this doesn't equal to $1$, we can do digital oversampling!
\end{itemize}

\begin{question}
    What exactly do band-limited signals look like? If $f$ is band-limited, how smooth is $f$? (Find out in Homework)
\end{question}

\begin{question}
    What if $f$ is not bandlimited (but smooth)? We can approximate
    \[f(t) \approx a \sum_{n \in \Zbb} f(an) \Phi_s(\frac{t}{a} - n)\]
    We expect the approximation to be good as $a \to 0$. Can you think of a bound of error for this? (This week's homework).
\end{question}

\newpage
\section{Lecture 16 - 3/3/2023}


Recall in the last lecture, we derived a new sampling formula
\[f(t) = a \sum_{n \in \Zbb} f(an) g(t - an)\]
where $g(t) = ...$ simplifies to get
\begin{align*}
    f(t) = \sum_{n \in \Zbb} f(an) \phi(t - an)\ (*)
\end{align*}
where $\phi(t) = \frac{\cos \Omega t - \cos \beta \Omega t}{\beta(\beta - 1)t^2\Omega^2}$ for $\beta > 1$. We made the observations:
\begin{itemize}
    \item $\lim_{\beta \to 1} \phi(t) = \frac{\sin \Omega t}{\Omega t} \implies $ Shannon
    \item By applying L'Hopital's Rule twice, we have that $\lim_{t \to 0} \phi(t) = \frac{1 + \beta}{2\beta}$. In particular, $\frac{1 + \beta}{2\beta} = 1$ if and only if $\beta = 1$. Thus, if $\beta > 1$, we can obtain a reconstruction formula by solving for $f(0)$ in $(*)$ when $(t = 0)$.
    \item We can thus use $(*)$ to find that
    \[f(0) = \frac{\beta}{2(\beta - 1)} \sum_{n \in \Zbb - \{0\}} f(\frac{\pi n}{\beta \Omega}) \frac{1}{\pi^2 n^2} [\cos \frac{\pi n}{\beta} - \cos \pi n]\]
    Observe that this formula decays close to $\frac{1}{n^2}$.
    \item This means that we can recover $f(0)$ without knowing what $f(0)$ is, by using the samples $\{f(\frac{\pi n}{\beta \Omega}\ :\ n \in \Zbb - \{0\}\}$.
\end{itemize}

\subsection{Filtering and Denoising}

A slightly different question is to ask - suppose we transmit the digital packet
    \[\{f(\frac{\pi n}{\beta \Omega})\ :\ n \in \Zbb\}\]
    but they are subject to random noise, so we have received
    \[\{f(\frac{\pi n}{\beta \Omega}) + \epsilon_n \ :\ n \in \Zbb\}\]
    where $\epsilon_n$ is random.

\begin{question}
    How well can we recover the original signal?
\end{question}

We can use $(*)$ to denoise! Let
\[f(t) = \sum_{n \in \Zbb} f(an) \phi(t - an)\]
Let $t = am$ for $m \in \Zbb$, we have that
\[f(am) = \sum_{n \in \Zbb} f(an) \phi(a(m - n))\]

\begin{definition}
Let $f^{noisy}$ be the function altered by noise, we define,
    $$f^{new}(am) \coloneqq \sum_{n \in \Zbb} f^{noisy}(an) \phi(a(m-n))$$
We think of $\phi(a(m-n))$ as a form of weighting and $f^{new}(am)$ as the wegithed sum of the noises.\\\\
Let's instead consider the packet $\{f^{new}(am): m \in \Zbb\}$. We expect the noise of this new function to be smaller (by APMA 1650 - taking an average always reduces the variance). We can refine the function by plugging $f^{new}(am)$ into the denoising function above again, and do it recursively down.
\end{definition}

\begin{remark}
Oversampling increases the bandwidth we can collect, but it actually impedes the smoothing process of the signal. So our denoising process really depends on how much of the bandwidth we are asking to sample over. If it's around the bandwidth of the original signal, we have a really good approximation. However, if the original bandwidth is unknown, this is a really difficult question on how to recover the original signal.    
\end{remark}

\begin{conjecture}
If we choose our filter such that the bandwidth is the exactly the bandwidth of the original signal, we can recover $\{f(\frac{\pi n}{\beta n}) + \epsilon_n: n \in \Zbb\}$ exactly (most of the time - there are some counter example, for example choose $\epsilon_n = 1$ for all $n$, this is not actually a noise but the probability of all $\epsilon_n$ being the same is really low ... but the question is, on average (the expected probability), you will always get the original signal). This is a research question that even the instructor does not know.
\end{conjecture}

\newpage
\section{Lecture 17 - 3/6/2023}

\subsection{Signal Spaces}
In the previous section, we showed that (under quite general conditions on $g$), that any band-limited signal can be represented digitally with:
\[f(t) = a \sum_{n \in \Zbb} f(an) g(t - an)\]
provided that $a \leq \frac{\pi}{\Omega}$ and
\[\widehat{g}(\omega) = \begin{cases}
    1,\ |\omega| \leq \Omega\\
    \text{anything , otherwise}
\end{cases}\]

\begin{definition}
    Fix $g(t - an)$ and $a$, This suggests that
    \[V_{g, a} \coloneqq \overline{\spn \{g(t - an)\ :\ n \in \Zbb\}}\]
    is a good space for representing signals digitally. Any signal can be represented by
    \[\sum_{n \in \Zbb} c_n g(t - an)\]
    for appropriate digital information $\{c_n: n \in \Zbb\}$. We call $V_{g, a}$ a \textbf{signal space}.
\end{definition}

\begin{definition}[Scalar Product on Signal Space]
There's a canonical scalar product on $V_{g, a}$:    
\[f, h \in V_{g, a} \mapsto (f, h)_* \coloneqq \int_{\Rbb} f(t) \overline{h}(t) dt\]
There's a question on whether this value is finite, but as long as we require our signal to have finite energy, this will be well-defined.
\end{definition}

\begin{question}
    Is the digital representation \textbf{stale}? That is, suppose $f \in V_{g, a}$ and we write
    \[f(t) = \sum_{n \in \Zbb} c_n g(t - an)\]
    We assume that the $c_n$ are uniquely determined (choose $g(t - an)$ linearly independent). This defines a bijective mapping
    \[f \mapsto \{c_n: n \in \Zbb\}\]
    This is a bijective map from $V_{g, a} \to \Cbb[\Zbb]$ (where $\Cbb[\Zbb]$ is thought of as the \textbf{sequence space} of countable length with coefficients in $\Cbb$).\\\\
    The question of staleness is - is the map continuous?
\end{question}

How are we going to define a topology on $\Cbb[\Zbb]$, we could choose the $\ell_\infty$ norm:
\[||\{c_n: n \in \Zbb\}|| \coloneqq \sup_{n \in \Zbb} |c_n| \sim \ell_\infty\]
But we wouldn't choose this, instead, let's consider the $\ell_2$ norm:
\[||\{c_n: n \in \Zbb\}|| \coloneqq (\sum_{n \in \Zbb} |c_n|^2)^{1/2} \sim \ell_2\]

To show that the math is continuous, we really want a constant map $A > 0, B > 0$ (finite) such that 
\[A ||\{c_n\} - \{\Tilde{c_n}\}|| \leq ||f - \Tilde{f}|| \leq B ||\{c_n\} - \{\Tilde{c_n}\}||\]
This is what's typically known as a \textbf{norm equivalence}. More generally, we have
\[A ||\{c_n\}|| \leq ||f|| \leq B||c_n||\ \forall f \in V_{g, a}\]
This is known as the \textbf{Riesz Basis Property}

\begin{remark}
    When / How could this Riesz Basis Property not hold?
    \[\includegraphics[width=0.5\textwidth]{Figures/lec17-1}\]
    We call this phenomenon \textbf{super sensitivity}. In particular, $A$ and $B$ only depends on the choice of bases $\{g(t - an)\}_{n \in \Zbb}$.
\end{remark}

Ideally, we want to have an \textit{orthonormal basis}, ie.
\[\int_\Rbb g(t - an) \overline{g(t - am)} dt = \begin{cases}
    1,\ \text{if $m = n$}\\
    0,\ \text{if $m \neq n$}
\end{cases}\]

\begin{proposition}
    If $\{g(t - an)\}$ is an orthonormal basis, then it satisfies the Riesz Basis Property 
\end{proposition}

\begin{proof}
In the case of orthonormal basis, for any
\[f(t) = \sum_{n \in \Zbb} c_n g(t - an)\]
We have that
\begin{align*}
    ||f||^2 &= (\sum c_n g(t - an), \sum c_m g(t - am))\\
    &= \sum_m \sum_n c_n \overline{c_m} (g(t - an), g(t - am))\\
    &= \sum_m |c_m|^2\\
    &= ||\{c_n\}||^2
\end{align*}
Choose $A = B = 1$, we have that
\[A ||\{c_n\}|| \leq ||f|| \leq B||c_n||\ \forall f \in V_{g, a}\]
\end{proof}

\begin{example}
    Consider the Shannon-Whittaker Basis, where
    \[g(t) = \phi_s(t) = \frac{\sin \pi t}{\pi t}\]
    We have two questions:
    \begin{enumerate}
        \item Is $\{g(t - an)\}$ an Riesz basis?
        \item Is $\{g(t - an)\}$ an orthonormal basis?
    \end{enumerate}
    Let $a = 1$, consider the integral below. We will evaluate this using Parserval's Identity:
    \begin{align*}
        \int_\Rbb \phi_s(t - n) \phi_s(t - m) dt
    \end{align*}
    What is the Fourier Transform of $t \mapsto \phi_s(t - m)$? This is the mapping:
    \begin{align*}
        \omega \mapsto \int_{\Rbb} e^{-i\omega t} \phi_s(t - m) dt &= e^{-i\omega m} \int_{-\infty}^\infty e^{-i\omega t} \phi_s(t) dt\\
        &= e^{-i\omega m} \cdot \begin{cases}
            1,\ |\omega| < \pi\\
            0,\ \text{otherwise}
        \end{cases}
    \end{align*}
    Let's call $\chi(\omega)$ the function $\begin{cases}
            1,\ |\omega| < \pi\\
            0,\ \text{otherwise}
        \end{cases}$. Then by Parserval's Identity, we have that
    \begin{align*}
            \int_\Rbb \phi_s(t - n) \phi_s(t - m) dt &=\frac{1}{2\pi} \int_{-\infty}^\infty e^{-i\omega n} \chi(\omega) e^{i\omega m} \overline{\chi(\omega)} d\omega\\
            &= \frac{1}{2\pi} \int_{-\pi}^\pi e^{i\omega (m - n)} d\omega\\
            &= \begin{cases}
                1,\ m = n\\
                0,\ m \neq n
            \end{cases}
    \end{align*}
    Hence, the answer to Question $(1)$ and $(2)$ are both true!
\end{example}

\begin{corollary}
    The Shannon-Whittaker Basis is an orthonormal basis, hence is a Riesz Basis with $A = B = 1$.
\end{corollary}

\newpage
\section{Lecture 18 - 3/8/2023}

\subsection{Best Approximations and Projections}
Suppose we want to represent a signal $f(t) \approx \sum_{n \in \Zbb} c_n \phi(t - an)$ with $t \in \Rbb$. In other words, we wish to approximate $f$ by an element of the signal space $V_{\phi, a} = Span\{\phi(t - an): n \in \Zbb\}$ and $\phi$ is some appropriate function.

\begin{question}
    How can we choose the $\{c_n\}$ to approximate this desired $f(t)$?
\end{question}

There are two options:
\begin{enumerate}
    \item We can consider \textit{digital sampling}, ie.
    \[c_n = f(an), n \in \Zbb\]
    then we have that
    \[f(t) \approx (V_a^\phi f)(t) \coloneqq \sum_{n \in \Zbb} f(an)\phi(t - an)\]
    Is this a good idea? Well, the approximation above is exact when $\phi$ is a Shannon function and $f$ is band-limited by $\Omega$, and $a \leq \frac{\pi}{\Omega}$.\\\\
    However, this may not be a good idea if any of these hypothesis are not satisfied.
    \item \textit{Best possible choice}:\\
    The idea here is to choose $\{c_n\}$ such that we minimize the error
    \[\varepsilon(f; \{c_n\}) = ||f(t) - \sum_{n \in \Zbb} c_n \phi(t - an)||^2\]
    Let's differentiate this term with respect to $c_j$ (let's assuming $f, c_n, \phi$ all real), hence
    \begin{align*}
        0 &= \frac{1}{2} \frac{\partial \varepsilon}{\partial c_j}\\
        &= \langle f(t) - \sum c_n \phi(t - an), \phi(t - aj) \rangle \tag*{The inner product}\\
        \iff \langle f(t), \phi(t - aj)\rangle &= \sum_{n} c_n \langle \phi(t - an), \phi(t - aj) \rangle\ \forall j \in \Zbb\\
        \iff \sum_n M_{jn} c_n = f_j\ \forall j \in \Zbb
    \end{align*}
    $f_j$ is defined as $\langle f(t), \phi(t - aj)\rangle $ and $M_{jn} \coloneqq \langle \phi(t - an), \phi(t - aj) \rangle$.\\\\
    This is equivalent to the problem
    \[M \Vec{c} = \Vec{f}\]
    where $M = [M_{jn}]$ and $\Vec{f} = [f_j]$ and $\Vec{c} = [c_n]$, where we ended up with an infinite matrix equation.\\\\
    Let's suppose that we have an orthonormal basis, then $M_{jn}$ is really the infinite dimensional identity matrix. In this case, we have that
    \[c_n = f_n = \langle f(t), \phi(t - an) \rangle\ \forall n \in \Zbb\]
    One should also use the second derivative test to check this is really a minimum, that's left as an exercise.  
\end{enumerate}

This gives us the following proposition:
\begin{proposition}
    The best approximation (optimal $c$'s) given by
    \[(L_a f)(t) \coloneqq \sum_{n \in \Zbb} c_n \phi(t - an)\]
    where either the $\{c_n\}$ determined by either
    \begin{itemize}
        \item If $M \Vec{c} = \Vec{f}$ $\quad (\star)$
        \item Furthermore, if $\{\varphi(t - an): n \in \Zbb\}$ is orthonormal, we have that
        \[c_n = \langle f(t), \phi(t - an) \rangle \]
    \end{itemize}
\end{proposition}

\begin{remark}
    Suppose $f$ is band-limited again by $\Omega$ and $a \leq \frac{\pi}{\Omega}$ with $\phi$ chosen to be Shannon Sampling function (or similar) again. What can we say about $c_n$ given by $(\star)$?\\\\
    They should actually coincide with $f(an)$ because the minimum of the optimization in Best Approximation is unique. We know $V^\phi_a f = f$ in this case and $||f - L_a f|| \leq ||f - V^{\phi}_a f|| = 0$, hence $L_a f = f - V^\phi_a f$.
\end{remark}

This leads us to suspect that if we choose $\varphi$ such that $\hat{\varphi}$ looks like the step function from $-\Omega$ to $\Omega$, ie. if
\begin{itemize}
    \item $\hat{\varphi}(0) = 1$ and $\hat{\varphi}'(0) = \hat{\varphi}''(0) = ... = \hat{\varphi}^{(m)}(0) = 0$ for $m$ as large as possible
\end{itemize}
then $\varphi$ should be a good sampling function.

\[\hat{\varphi}(\omega) \approx 1 \text{ for } |\omega| \text{ in some range}\]

These are the conditions for a \textbf{Butterworth filter}, and, in this case, the best approximation $L^a f(t)$ is about the same as $(V^\phi_a f)(t)$. Hence, we can use Digital Sampling as a ``good enough" approximation.

\begin{theorem}
    Suppose that
    \begin{itemize}
        \item (a) $\{\phi(t - an): n \in \Zbb\}$ is orthonormal
        \item (b) $\hat{\phi}(\omega) \approx 1$ for $|\omega| \leq \Omega$
    \end{itemize}
    Then for $f$ bandlimited by $\Omega$, we have that
    \[V^\phi_a f \approx L_a f\ \text{for } a \leq \pi/\Omega\]
\end{theorem}

\begin{proof}
By Parserval's Identity, 
    \begin{align*}
        c_k^{\star} &\coloneqq \int_{-\infty}^\infty f(t) \phi(t - ak)\ dt\\
        &= \frac{1}{2\pi} \int_{-\infty}^\infty \widehat{f}(\omega) e^{i\omega k a} \overline{\hat{\phi}(\omega)} d\omega\\
        &= \frac{1}{2\pi} \int_{-\Omega}^\Omega \widehat{f}(\omega) e^{i\omega k a} \overline{\hat{\phi}(\omega)} d\omega\\
        &\approx \frac{1}{2\pi}  \int_{-\Omega}^\Omega \widehat{f}(\omega) e^{i\omega k a} d\omega \tag*{Since $\hat{\phi}(\omega) \approx 1$}\\
        &= f(ka) \tag*{Definition of Inverse Fourier Transform}
    \end{align*}
\end{proof}


\newpage
\section{Lecture 19 - 3/10/2023}
\subsection{Scaling and Two Scale Relation}

We have proposed to use spaces of form
\[V_{\phi, a} = \spn \{\phi(t - an): n \in \Zbb\}\]
to represent signals in the form
\[f(t) \approx \sum_{n \in \Zbb} c_n \phi(t - an)\]
provides that $\hat{\phi}$ satisfies same assumptions. If $a \to 0$, we expect the approximations to improve. This is because the Bandwidth $\Omega$ can grows as $\pi/a$, so we would get more information about the signal.\\

We have also shown that choosing $c_n = f(an)$ is a near optimal choice (under reasonable circumstances on $f$), a question we are now interested in is
\begin{question}
    How to choose $a$? When do we know that $a$ is sufficiently small?
\end{question}

We know that we have a good approximation when $\Omega \leq \frac{\pi}{a}$! But what if $f$ has a wider bandwidth? In the previous lectures, we have doubled the sampling rate by changing $a \mapsto a/2$ so we can represent the bandwidth up to $\Omega \leq \frac{2\pi}{a}$. In general, we can keep doubling the sampling rate by sending $a \mapsto a/2^n$ to find a sufficiently high bound on the bandwidth.\\

To adjust the bound on $\Omega$, we are in effect using a family of signal spaces of the form ($a = 1$, for convenience):
\[V_n = V_{\phi, 2^{-n}} = \spn \{\phi(2^n t - k)\ |\ k \in \Zbb\}\]
for $n \in \Zbb$. (Note that we can rewrite $\phi(t - an)$ as $\phi(\frac{t}{a} - n)$ up to some reparametrization).\\

The issue is that we not have an infinite signal spaces.

\begin{remark}
    Suppose $\phi$ is real-valued with center $t_* = 0$ and spread $\Delta_\phi$, then the map $t \mapsto \phi(2^n t - k)$ would satisfy:
    \begin{itemize}
        \item For energy, we have that $\int_{-\infty}^\infty |\phi(2^n t - k)|^2 dt = 2^{-n} \int_{-\infty}^\infty |\phi(s)|^2 ds$ by consider the substitution $s = 2^n t - k$.
        \item For center, we have that
        \begin{align*}
        \int_{-\infty}^\infty t|\phi(2^n t - k)|^2 dt &= 2^{-n} \int_{-\infty}^\infty |\phi(s)|^2 2^{-n} (k + s) ds \tag*{Same Substituion}\\
        &= 2^{-2n} k \int_{-\infty}^\infty |\phi(s)|^2 ds \tag*{Since $t^* = 0$}\\
        &= 2^{-n} k \int_{-\infty}^\infty |\phi(2^n t - k)|^2 dt
        \end{align*}
        Hence, $t \to \phi(2^n t - k)$ has center at $2^{-n} k$.
        \item For radius, we have that
        \begin{align*}
            \int_{-\infty}^\infty (t - 2^{-n} k)^2 |\phi(2^n t - k)|^2 dt &= ...
        \end{align*}
        Since this is a Friday morning, the instructor decided to not calculate the radius but heuristically guess what the radius is. But we note that the multiplication by $2^n$ in $\phi(2^n t - k)$ would make the signal a lot tighter, so its radius should be smaller. We eventually find that
        \[t \mapsto \phi(2^n t - k) \text{ has radius } 2^{-n} \Delta_\phi\]
    \end{itemize}
    In Summary, we compare the two signals $\phi(t)$ and $\phi(2^n t - k)$ as follows:
    \[\includegraphics[width=0.7\textwidth]{Figures/lec19-p1.png}\]
    This means that if we have an expansion of the form
    \[V_n \ni f(t) = \sum_{k \in \Zbb} c_{n, k} \phi(2^n t - k)\]
    then $c_{n, k}$ is gicing information about $f$ in the neighborhood of the point $t = 2^{-n} k$ and a region of size $\sim 2^{-n} \Delta_\phi$. This gives us the expectation that
    \[c_{n, k} \sim f(2^{-n} k) = f(ak)\]
    where $a = 2^{-n}$ and $2^{-n} k = ak$. Hence we really do great better resolution by considering the family $\{V_n\}_{n \in \Zbb}$
\end{remark}

This is all very well, but we still need an infinite number of spaces! But here's a thought - are the $\{V_n\}_{n \in \Zbb}$ really all independent?

\begin{example}
    Let's choose $\phi(t) = \begin{cases}
        1,\ t \in [0, 1]\\
        0,\ \text{otherwise}
    \end{cases}$, then $\phi(t)$ and $\phi(2^n t - k)$ looks like:
\[\includegraphics[width=0.5\textwidth]{Figures/lec19-p2.png}\]
    We observe that $V_0 = \spn \{\phi(t - k):\ k \in \Zbb\}$ and $V_n = \spn \{\phi(2^n t - k):\ k \in \Zbb\}$
    A function in $V_0$ and $V_n$ look like
\[\includegraphics[width=0.5\textwidth]{Figures/lec19-p3.png}\]
    Judging from the image, it seems plausible that $V_0 \subset V_n$ for $n \geq 0$.
\end{example}

More generally, iterating this relation, we have the following claim
\begin{proposition}
When $\phi(t) = \begin{cases}
        1,\ t \in [0, 1]\\
        0,\ \text{otherwise}
    \end{cases}$, the family $\{V_n: n \in \Zbb\}$ is nested:
\[...\ V_{-2} \subset V_{-1} \subset V_0 \subset V_1 \subset V_2 \subset ...\]
\end{proposition}

The proposition above is not true in general! Hence, there's something special about this choice of $\phi(t)$ that makes it true. What's special about $\phi(t)$ is that $\phi(t)$ is a linear combination of $\phi(2t - k)$ for $k \in \Zbb$:
    \[\phi(t) = \phi(2t - 1) + \phi(2t)\]
This is what's called a \textbf{Two-Scale Relation}.\\

\begin{definition}
More generally, if we can find scalars $\{ p_j \}$ such that
\[\phi(t) = \sum_{j \in \Zbb} p_j \phi(2t - j) \]
Then we say that $\phi$ satisfies a \textbf{Two- Scale Relation}.
\end{definition}

\begin{theorem}
    If $\phi(t)$ satisfies a 2-scale relation, then $\{V_n\}$ are nested.
\end{theorem}

We will prove this theorem next time.


\end{document}
